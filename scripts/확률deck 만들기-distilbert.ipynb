{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/h/hev/log-analytics\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import multiprocessing\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pytorch_transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AlbertForSequenceClassification,\n",
    "    AlbertTokenizer,\n",
    "    DebertaForSequenceClassification,\n",
    "    DebertaTokenizer,\n",
    "    SqueezeBertTokenizer,\n",
    "    SqueezeBertForSequenceClassification,\n",
    "    XLNetTokenizer,\n",
    "    XLNetForSequenceClassification,\n",
    ")\n",
    "\n",
    "from datasets import load_test_data, load_train_data, MyDataset\n",
    "from utils import SAM, AverageMeter, CustomLogger, FocalLoss, seed_everything\n",
    "\n",
    "from main import MyTrainer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(deck, feat, topk):\n",
    "    dist = torch.norm(deck - feat[None], dim=1, p=None)\n",
    "    knn = dist.topk(topk, largest=False)\n",
    "    values, indices = knn\n",
    "    return values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/distilbert-base-uncased.yaml\", \"r\") as f:\n",
    "    C = EasyDict(yaml.load(f, yaml.FullLoader))\n",
    "    C.result_dir = Path(C.result_dir)\n",
    "    C.dataset.dir = Path(C.dataset.dir)\n",
    "    seed_everything(C.seed, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'name': 'distilbert-base-uncased'},\n",
       " 'comment': None,\n",
       " 'result_dir': PosixPath('results/distilbert-base-uncased'),\n",
       " 'debug': False,\n",
       " 'seed': 20210425,\n",
       " 'train': {'SAM': False,\n",
       "  'folds': [1],\n",
       "  'checkpoints': [None],\n",
       "  'loss': {'name': 'focal', 'gamma': 2},\n",
       "  'optimizer': {'name': 'AdamW'},\n",
       "  'finetune': {'do': True, 'step1_epochs': 2, 'step2_epochs': 4},\n",
       "  'max_epochs': 10,\n",
       "  'lr': 1e-05,\n",
       "  'scheduler': {'name': 'ReduceLROnPlateau',\n",
       "   'params': {'factor': 0.5, 'patience': 3, 'verbose': True}}},\n",
       " 'dataset': {'dir': PosixPath('data/ori'), 'batch_size': 30, 'num_workers': 8}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained results/distilbert-base-uncased/distilbert-base-uncased-focal-AdamW-lr1e-05_1.pth\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(C, 1, \"results/distilbert-base-uncased/distilbert-base-uncased-focal-AdamW-lr1e-05_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f6914805210>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer.model\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = []\n",
    "\n",
    "\n",
    "def hook(model, input, output):\n",
    "    activation.append(output.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f690c079290>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pre_classifier.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 데이터 deck 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 378377/378377 [19:32<00:00, 322.64it/s]\n"
     ]
    }
   ],
   "source": [
    "activation = []\n",
    "tlevels = []\n",
    "plevels = []\n",
    "with tqdm(total=len(trainer.tdl.dataset), ncols=100, file=sys.stdout) as t:\n",
    "    for id, text, tlevel, otext in trainer.tdl:\n",
    "        tlevels.append(tlevel)\n",
    "        plevel = model(text.cuda())[0].cpu()\n",
    "        plevels.append(plevel)\n",
    "        t.update(len(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 94595/94595 [04:52<00:00, 323.34it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(trainer.vdl.dataset), ncols=100, file=sys.stdout) as t:\n",
    "    for id, text, tlevel, otext in trainer.vdl:\n",
    "        tlevels.append(tlevel)\n",
    "        plevel = model(text.cuda())[0].cpu()\n",
    "        plevels.append(plevel)\n",
    "        t.update(len(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = {\"feat\": torch.cat(activation), \"tlevel\": torch.cat(tlevels), \"fclevel\": torch.cat(plevels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([472972, 768]), torch.Size([472972]), torch.Size([472972, 7]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deck[\"feat\"].shape, deck[\"tlevel\"].shape, deck[\"fclevel\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(deck, \"results/distilbert-base-uncased/distilbert-base-uncased-focal-AdamW-lr1e-05_1-deck.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
