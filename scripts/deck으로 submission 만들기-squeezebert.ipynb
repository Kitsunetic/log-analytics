{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9499b2ec-6ebb-4575-b647-f2946b3eb8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shim/cev/dl/log-analytics\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23a629d-38d6-4439-89a6-272d68eefbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import multiprocessing\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_optimizer\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pytorch_transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AlbertForSequenceClassification,\n",
    "    AlbertTokenizer,\n",
    "    DebertaForSequenceClassification,\n",
    "    DebertaTokenizer,\n",
    "    SqueezeBertTokenizer,\n",
    "    SqueezeBertForSequenceClassification,\n",
    "    XLNetTokenizer,\n",
    "    XLNetForSequenceClassification,\n",
    ")\n",
    "\n",
    "from datasets import load_test_data, load_train_data, MyDataset, load_train_total_data\n",
    "from utils import SAM, AverageMeter, CustomLogger, FocalLoss, seed_everything\n",
    "\n",
    "from main import MyTrainer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45261ba-46c7-4bc8-b81c-c7376ddab540",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/squeezebert-uncased.yaml\", \"r\") as f:\n",
    "    C = EasyDict(yaml.load(f, yaml.FullLoader))\n",
    "    C.result_dir = Path(C.result_dir)\n",
    "    C.dataset.dir = Path(C.dataset.dir)\n",
    "    seed_everything(C.seed, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf54770-ac64-4a8c-8272-e7c57414e7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'name': 'squeezebert/squeezebert-uncased'},\n",
       " 'comment': None,\n",
       " 'result_dir': PosixPath('results/squeezebert-uncased'),\n",
       " 'debug': False,\n",
       " 'seed': 20210425,\n",
       " 'train': {'SAM': False,\n",
       "  'folds': [4],\n",
       "  'checkpoints': [None],\n",
       "  'loss': {'name': 'focal', 'gamma': 2},\n",
       "  'optimizer': {'name': 'AdamW'},\n",
       "  'finetune': {'do': True, 'step1_epochs': 3, 'step2_epochs': 5},\n",
       "  'max_epochs': 10,\n",
       "  'lr': 1e-05,\n",
       "  'scheduler': {'name': 'ReduceLROnPlateau',\n",
       "   'params': {'factor': 0.5, 'patience': 3, 'verbose': True}}},\n",
       " 'dataset': {'dir': PosixPath('data/ori'), 'batch_size': 20, 'num_workers': 8}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2a935b-d7c7-4925-ac6d-da53e92c52bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at squeezebert/squeezebert-uncased were not used when initializing SqueezeBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing SqueezeBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SqueezeBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SqueezeBertForSequenceClassification were not initialized from the model checkpoint at squeezebert/squeezebert-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1.pth\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(\n",
    "    C, 1, \"results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b02e824-3a56-4b36-ac93-ed1de94aeb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f23843b9c90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer.model\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cdf5bf-f471-4f51-938b-66fa588848d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeBertForSequenceClassification(\n",
       "  (transformer): SqueezeBertModel(\n",
       "    (embeddings): SqueezeBertEmbeddings(\n",
       "      (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): SqueezeBertEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): SqueezeBertModule(\n",
       "          (attention): SqueezeBertSelfAttention(\n",
       "            (query): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (key): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (value): Conv1d(768, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "            (matmul_qk): MatMulWrapper()\n",
       "            (matmul_qkv): MatMulWrapper()\n",
       "          )\n",
       "          (post_attention): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): ConvActivation(\n",
       "            (conv1d): Conv1d(768, 3072, kernel_size=(1,), stride=(1,), groups=4)\n",
       "          )\n",
       "          (output): ConvDropoutLayerNorm(\n",
       "            (conv1d): Conv1d(3072, 768, kernel_size=(1,), stride=(1,), groups=4)\n",
       "            (layernorm): SqueezeBertLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): SqueezeBertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe7dae8-22c9-4426-b112-e1f47a41d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = []\n",
    "\n",
    "\n",
    "def hook(model, input, output):\n",
    "    activation.append(output.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6490b23a-2146-4360-9e19-90f543a715ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f23843b9750>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.pooler.dense.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50367e9c-bfa6-4dc0-9d72-6562fb73b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = load_train_total_data(C.dataset.dir, trainer.tokenizer, 100, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d48cc-9fc9-418d-a27c-4bfd94d7956d",
   "metadata": {},
   "source": [
    "## tdeck 만들기 (모든 train 데이터에 대한 집합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300c3cbb-2ebd-4cfc-b976-d39f0b2c7397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 472972/472972 [42:06<00:00, 187.18it/s]\n"
     ]
    }
   ],
   "source": [
    "activation = []\n",
    "deck = {\n",
    "    \"fcfeat\": [],\n",
    "    \"tlevel\": [],\n",
    "    \"fclevel\": [],\n",
    "    \"otext\": [],\n",
    "}\n",
    "with tqdm(total=len(dl.dataset), ncols=100, file=sys.stdout) as t:\n",
    "    for id, text, tlevel, otext in dl:\n",
    "        pred = model(text.cuda(non_blocking=True))[0].cpu()\n",
    "        deck[\"fcfeat\"].append(pred)\n",
    "        deck[\"tlevel\"].append(tlevel)\n",
    "        deck[\"fclevel\"].append(pred.argmax(dim=1))\n",
    "        deck[\"otext\"].extend(otext)\n",
    "\n",
    "        t.update(len(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2c86c1d-94e2-4e23-8d89-41db94deeda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"fcfeat\"] = torch.cat(deck[\"fcfeat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d3c467-702b-4968-9ed1-13664cd8c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"tlevel\"] = torch.cat(deck[\"tlevel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e95b0650-5b9c-4cd6-8dcf-0c1c64c49928",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"fclevel\"] = torch.cat(deck[\"fclevel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9593f1db-f032-4c6f-b375-4b257138c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"feat\"] = torch.cat(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acab1db-357d-40ee-b0f7-36e253e957b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fcfeat', 'tlevel', 'fclevel', 'otext', 'feat'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deck.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68713498-bda5-44d7-9581-04fc3189ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    \"results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1-deck1.npz\",\n",
    "    fcfeat=deck[\"fcfeat\"].numpy(),\n",
    "    tlevel=deck[\"tlevel\"].numpy(),\n",
    "    fclevel=deck[\"fclevel\"].numpy(),\n",
    "    feat=deck[\"feat\"].numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0352b-2f18-4a06-8747-1c6e59288c19",
   "metadata": {},
   "source": [
    "## sdeck 만들기 (모든 test 데이터에 대한 집합)\n",
    "\n",
    "reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08eba62f-3a09-4766-8eed-fed917b7246f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1418916/1418916 [2:16:34<00:00, 173.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# sfeats 저장\n",
    "activation = []\n",
    "deck = {\"fcfeat\": [], \"fclevel\": [], \"otext\": []}\n",
    "with tqdm(total=len(trainer.dl_test.dataset), ncols=100, file=sys.stdout) as t:\n",
    "    for _, text, otext in trainer.dl_test:\n",
    "        pred = model(text.cuda(non_blocking=True))[0].cpu()\n",
    "        deck[\"fcfeat\"].append(pred)\n",
    "        deck[\"fclevel\"].append(pred.argmax(dim=1))\n",
    "        deck[\"otext\"].extend(otext)\n",
    "        t.update(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36c9bf51-1aee-4918-86d0-1beaca8237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"fcfeat\"] = torch.cat(deck[\"fcfeat\"])\n",
    "deck[\"fclevel\"] = torch.cat(deck[\"fclevel\"])\n",
    "deck[\"feat\"] = torch.cat(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aae3694-1d6f-42b8-acd4-329b7a924393",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    \"results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1-deck2.npz\",\n",
    "    fcfeat=deck[\"fcfeat\"].numpy(),\n",
    "    fclevel=deck[\"fclevel\"].numpy(),\n",
    "    feat=deck[\"feat\"].numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2309f9d-6bd6-4a9f-a7f6-d17e555afd92",
   "metadata": {},
   "source": [
    "## dist값, KNN level, FC level 을 저장\n",
    "\n",
    "여기서 reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2ae09b-a4df-4ca4-8925-6871d6c30830",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck1 = np.load(\n",
    "    \"results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1-deck1.npz\"\n",
    ")\n",
    "deck2 = np.load(\n",
    "    \"results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1-deck2.npz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1aa53bc-5f01-440d-b0a3-b8733f8fb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdeck = {\n",
    "    \"feat\": torch.from_numpy(deck1[\"feat\"]).cuda(),\n",
    "    \"tlevel\": torch.from_numpy(deck1[\"tlevel\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63954ca9-50e9-4bbd-8fe1-f1e9847b477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdeck = {\n",
    "    \"feat\": torch.from_numpy(deck2[\"feat\"]).cuda(),\n",
    "    \"fcfeat\": torch.from_numpy(deck2[\"fcfeat\"]),\n",
    "    \"fclevel\": torch.from_numpy(deck2[\"fclevel\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6c1fd0-cb62-4a98-9eaf-1ae6f1773efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([472972, 768]), torch.Size([1418916, 768]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdeck[\"feat\"].shape, sdeck[\"feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d2e16c-36f7-496e-b7b8-1fedfa4dea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(deck, feat, topk):\n",
    "    dist = torch.norm(deck - feat[None], dim=1, p=None)\n",
    "    values, indices = dist.topk(topk, largest=False)  # knn\n",
    "    return values, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514c7e80-2a81-4513-8b34-cca5eb68e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0308, 0.0328, 0.0368, 0.0369, 0.0370, 0.0381, 0.0381, 0.0382],\n",
      "       device='cuda:0')\n",
      "tensor([324718, 252143, 449088, 356215,  83123, 329544, 159483, 327411],\n",
      "       device='cuda:0')\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# 실험\n",
    "values, indices = get_dist(tdeck[\"feat\"], sdeck[\"feat\"][2], 8)\n",
    "print(values)\n",
    "print(indices)\n",
    "print(tdeck[\"tlevel\"][indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a69f3ab-334a-44ff-8a80-880f13bdd403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1418916/1418916 [3:04:35<00:00, 128.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# dist를 구함\n",
    "dists, indices, fcfeats, tlevels = [], [], [], []\n",
    "with tqdm(total=len(sdeck[\"feat\"]), ncols=100, file=sys.stdout) as t:\n",
    "    for i in range(len(sdeck[\"feat\"])):\n",
    "        dist_, index_ = get_dist(tdeck[\"feat\"], sdeck[\"feat\"][i], 8)\n",
    "        dist = dist_.cpu()\n",
    "        index = index_.cpu()\n",
    "        fcfeat = sdeck[\"fcfeat\"][i]\n",
    "        tlevel = tdeck[\"tlevel\"][index]\n",
    "        dists.append(dist)\n",
    "        indices.append(index)\n",
    "        fcfeats.append(fcfeat)\n",
    "        tlevels.append(tlevel)\n",
    "\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "456f0a88-10a4-4122-8449-d09ac82e27dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " tensor([3201, 2550, 1541, 2047,  406,  215,  521,  748]),\n",
       " tensor([ 4.4087, -1.7369, -3.3400, -2.2295, -4.4363, -2.2275, -3.1217]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists[0], indices[0], fcfeats[0], tlevels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "272be847-9cde-49c2-9b50-e23e255f4484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8]), torch.Size([8]), torch.Size([7]), torch.Size([8]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists[0].shape, indices[0].shape, fcfeats[0].shape, tlevels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c71ad7c8-e92e-4d5c-97b6-a39ab7505418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_ = torch.stack(dists)\n",
    "indices_ = torch.stack(indices)\n",
    "fcfeats_ = torch.stack(fcfeats)\n",
    "tlevels_ = torch.stack(tlevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7a0c2b2-b414-4398-9dda-ea457df15b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    \"results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1-deck3.npz\",\n",
    "    dists=dists_.numpy(),\n",
    "    indices=indices_.numpy(),\n",
    "    fcfeats=fcfeats_.numpy(),\n",
    "    tlevels=tlevels_.numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda26b12-1ac4-4abb-9f91-98f93cbf10af",
   "metadata": {},
   "source": [
    "## deck결과 연구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd110496-1db5-45ae-9598-f3cfdf639f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = load_train_total_data(C.dataset.dir, SqueezeBertTokenizer.from_pretrained(C.model.name), 100, 6)\n",
    "ds = dl.dataset\n",
    "df = pd.read_csv(\"data/ori/test.csv\")\n",
    "tdf = pd.read_csv(\"data/ori/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22310d98-179c-4e4d-bc21-1eb6c3594a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck3 = np.load(\"results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1-deck3.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5be97d-a6e5-416b-ae5e-68f30208f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck3 = {k: torch.from_numpy(v) for k, v in deck3.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "291ec0fc-db6a-4f26-897c-91ac982ab5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck3[\"fclevels\"] = deck3[\"fcfeats\"].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d76fb19-f59a-42f1-a799-0200d0c6d11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dists': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0308, 0.0328, 0.0368,  ..., 0.0381, 0.0381, 0.0382],\n",
       "         ...,\n",
       "         [0.0237, 0.0278, 0.0351,  ..., 0.0372, 0.0389, 0.0396],\n",
       "         [0.0228, 0.0236, 0.0245,  ..., 0.0262, 0.0279, 0.0287],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " 'indices': tensor([[  3201,   2550,   1541,  ...,    215,    521,    748],\n",
       "         [    17,     13,      8,  ...,      0,      3,      6],\n",
       "         [324718, 252143, 449088,  ..., 329544, 159483, 327411],\n",
       "         ...,\n",
       "         [206732,  10751, 222608,  ..., 145230, 221758, 216741],\n",
       "         [153755, 387601, 124924,  ...,  97088, 177289, 290495],\n",
       "         [    17,     13,      8,  ...,      0,      3,      6]]),\n",
       " 'fcfeats': tensor([[ 4.4087, -1.7369, -3.3400,  ..., -4.4363, -2.2275, -3.1217],\n",
       "         [ 4.4719, -1.7817, -3.2795,  ..., -4.4334, -2.2657, -3.2150],\n",
       "         [-1.6823,  4.6369, -4.0892,  ..., -4.3544, -1.6090, -3.6748],\n",
       "         ...,\n",
       "         [-1.6587,  4.6512, -4.1571,  ..., -4.4218, -1.6140, -3.7444],\n",
       "         [ 4.7360, -1.8455, -3.6387,  ..., -5.1526, -2.0866, -3.9665],\n",
       "         [ 4.4719, -1.7817, -3.2795,  ..., -4.4334, -2.2657, -3.2150]]),\n",
       " 'tlevels': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'fclevels': tensor([0, 0, 1,  ..., 1, 0, 0])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deck3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984cc85b-2ff3-4416-ad6e-105c3ba311fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274395 536615 812987 7890 1170801 349066 586315 892213 1239920 1392979 706410 93934 1019475 648933 340297 956511 329693 247539 160810 1413155 1271251 737453 81139 384420 199860 77440 1085297 1223462 171740 427956 64268 1131555 672208 660701 1313835 1007511 581422 451841 100875 968749 503551 507397 792421 1059158 1093095 1405043 843241 1132003 776033 1321286 85474 763582 290496 276564 880670 667667 1090762 258420 268382 848132 741654 1373814 1284064 398587 82287 649135 539834 153891 214469 521295 1224816 711251 1356744 182445 1213976 122720 1014018 227078 1350761 102363 877420 963100 671198 1030009 1213279 6003 1118924 1257904 147553 1111068 138723 594674 829306 1391989 1376627 926229 526783 51953 784227 633250 64630 66649 538051 22246 1375881 1293703 146134 1001613 1169229 363378 229188 68203 1101250 764979 594972 179014 1400122 148482 547899 872936 521634 184448 540106 397914 933984 631395 451943 170457 607089 134165 464615 337847 652696 1388489 725055 583008 82047 1335932 1229763 690800 236293 1138724 780579 861479 1168645 476759 1166153 119211 1285137 878423 30852 587580 1099425 868277 152604 1213127 329118 1071846 991686 680650 325665 11515 351474 1394210 647001 498279 599882 58126 623885 1241943 647174 562910 1041632 486253 277136 957952 989175 498548 555595 1016614 1258732 484782 103617 222456 1292011 1205019 422945 475295 465919 159643 175138 56488 237028 871076 117614 994333 606467 1028956 89806 782557 914875 392144 509918 1343974 301846 374788 484050 834516 555778 860037 893673 1374647 741201 846523 440770 774349 1310723 554720 706658 563330 1116409 158447 229252 460121 358839 584620 519388 685580 84329 971404 240023 471875 98325 918705 498019 1320714 538927 203669 1165787 133911 1053197 1106059 1025057 492659 1136864 148979 280846 1243165 342753 79586 1093714 833900 725843 1267055 778415 618541 324320 513531 719543 903460 1363903 639179 1183349 53736 612374 656900 1060154 724797 903635 1348825 282414 1001476 1378228 925383 523049 1268785 262584 245082 905187 1156752 1224682 589640 113459 984313 391260 522021 210211 507487 97456 1282788 775976 409965 231798 510951 15160 265564 994160 58797 169905 61121 262035 69985 50899 147281 1038198 1344909 1249219 201568 698144 1053830 842610 1046534 765566 146998 1314694 1036955 1393580 1290929 35574 557779 1003823 385750 919772 439730 4859 1246008 229259 27854 403988 964905 713456 1067324 1350456 970288 117120 476201 1407100 672860 209567 1107806 715879 1113524 1136985 1352554 996381 38592 855754 714618 921733 476523 143352 1367641 885168 208260 891851 470928 257809 391189 417302 380935 259982 692950 16932 1095797 317339 1015675 1236527 1367390 1394339 385118 749737 619525 934699 1262995 1312336 1007273 567692 988839 376317 565883 1063862 1195421 1249087 307082 1032818 520272 878261 595399 1205539 1333438 875404 500467 569985 1150665 435450 521922 855387 882429 516168 615806 892846 347972 180708 340793 500413 219706 1138221 194715 28909 1074264 182070 124322 790018 1394926 714753 1217211 320296 1150907 696809 1173686 1107166 755403 115577 245528 1101340 469389 917515 144118 407691 74595 995439 1271992 664220 1072834 1123299 445995 1063191 1095569 402920 444239 585641 939642 1281961 210933 827818 536955 455159 220621 877866 712003 634266 291672 1018826 834786 457910 797416 250568 729501 1402946 793150 249810 202879 1353447 663964 1325705 175492 411340 1340930 632377 704320 1360862 244738 916495 405938 736442 361035 783997 1055660 600542 216141 203311 748448 641117 375285 265925 1208677 971301 1380802 885100 65963 272112 374245 51887 373900 1094642 705441 1380764 1411263 859862 424928 50486 37805 1112548 864016 746289 1144478 554385 746829 1022137 1146502 368924 1001509 599639 100362 144966 770113 1244749 888930 225334 769057 1071487 229152 142588 950402 1010603 518463 153382 1390564 730114 19574 475616 694212 530379 282512 949998 1410455 675 290032 1244645 1147494 1206145 1090035 1101795 215791 710035 592449 317701 1271129 1020319 1025407 81118 897325 516736 316368 381185 491155 167531 529534 1303694 41200 323664 1022341 100663 449492 177433 904075 1016877 50747 38180 878828 358072 612885 219803 920394 459701 506504 2878 778531 1231764 966531 182233 840146 789810 887903 463531 672616 177619 305976 972324 909404 238333 1303811 937013 723219 683371 1186929 634373 299378 367002 1259600 168164 504294 1016469 709312 65387 927382 1111058 278138 774240 678825 589576 9819 1295093 734258 1319339 405379 703394 597981 220042 207506 74637 190307 442860 21125 1288254 706883 119759 485187 803631 830753 63380 1315846 70089 699735 1136784 198598 455381 1086224 1195876 179688 1147646 100100 1317185 1201878 148037 433156 137872 752894 1354562 1356198 1289695 362550 1235230 338146 1040891 1036247 865048 1318463 1345450 24623 731424 1289691 774347 1209465 524855 447118 265998 462932 455392 108513 4774 174608 1346149 1117131 451930 323964 1107150 982297 1078715 738649 818149 897437 316400 1222198 462289 1285687 850563 1374386 790980 51517 964929 950824 966680 376952 283170 973361 274595 351641 200861 131536 775158 853594 724929 1162587 354274 885269 324628 592496 275421 351430 1148519 296054 488060 1296733 1188886 1386509 1091680 823869 1130130 479453 247265 190392 180915 1233160 250376 887352 421952 498147 535612 379741 120492 330042 427044 447285 1312181 270174 947837 210592 748100 739950 662381 433878 136006 639625 165543 661477 828983 26497 980020 142103 540454 686570 1175366 97290 883760 196216 348086 1161348 982594 208382 878363 1327037 1324479 900684 188931 725175 1317024 561085 1174922 193314 632375 1018564 895050 689722 924928 661695 706075 708152 942277 1200513 68300 86017 361415 813241 599729 355929 1111468 968500 439941 986930 670963 755320 360515 1380816 1267226 411194 116312 913432 1075055 487726 613148 1179074 1140035 399430 464309 66248 79909 948013 507906 1032137 1248775 265838 779386 1398587 1189204 124131 1365521 54247 12250 27112 76838 373288 567960 722257 72520 1411380 550344 765739 286782 400172 821126 673688 758508 382977 930198 1152762 912728 996696 980229 1240629 1401434 1278552 541624 863077 1197207 745037 649234 1205409 411871 868865 1209119 706590 449717 287035 515724 1336665 1252868 1294230 1224515 133353 905416 942414 993101 780270 867344 606000 1361861 843165 967593 976744 545893 574217 843899 1203974 1137574 633514 618158 1302478 583215 1203048 436036 202225 821222 469602 1318256 1119216 660329 88037 643464 1272121 1007387 714457 155405 640914 1005238 236428 517041 1766 881808 1206032 578423 1414641 1007266 828468 1350459 1263788 520695 851235 166889 398467 1081129 64331 341272 341389 625811 388615 119436 1192503 981672 626187 1144850 177091 304176 811197 1274229 1032213 753043 849000 1390649 1206412 952456 964127 441586 1307961 1093308 673715 675116 1078436 707292 523936 232608 551658 253403 1268573 109289 150294 702466 382824 173163 1062268 817005 767130 470705 381135 515078 82598 489227 574853 499589 145541 191204 1057479 40053 414297 957827 870114 1391569 1204557 410461 369484 837164 1101725 11641 477551 712687 1216219 890324 176574 1097046 600621 1257608 230514 220041 1194448 57021 422935 983855 1031594 413708 590727 429994 885797 921279 713633 1409233 1266767 1412276 171281 1135223 730604 610590 72054 1259260 1178825 738767 461797 585731 1404810 1388961 1120882 1109724 699681 306502 709951 1216499 219809 1189548 978666 534052 127543 244133 91343 369230 353248 180693 719817 1323770 1182838 641537 1306951 1038346 575022 1017475 688145 567494 1362958 164604 742996 226853 1093890 839319 177589 156913 1306240 527606 371882 1226035 712002 723100 826640 317078 318614 224151 1410546 467766 1266997 317731 762947 362181 545826 1009694 540002 431062 167723 1380388 1049305 77934 206221 903556 328960 546306 1172244 393569 1415911 253325 664428 249443 1269865 356723 111399 453779 3780 464574 818619 322792 13992 53516 858920 70624 1367021 626048 922851 208964 1406561 1027286 164582 810575 835012 793172 390435 1092088 939757 1262133 394085 451446 148086 590802 1043759 2880 331666 284481 510480 1247566 1409316 658857 913183 680087 1379105 661892 124398 247874 236519 1259123 360894 273182 337352 74556 118018 464388 560070 267189 1037710 208421 932131 667610 1074618 1261132 886400 126889 918706 1073593 682964 435378 1154848 283917 805175 573214 996951 648014 19064 341099 377311 1385391 863210 50292 747325 610731 1219198 826989 747681 1298424 1078950 1286610 16975 1348912 224506 837140 168202 486693 278695 393803 1279626 919038 287952 422001 1360601 1400531 658510 1272603 914744 571365 528576 437543 540802 333032 34161 1370911 864774 806053 684383 758823 1146907 811683 651521 1146551 353821 648651 322477 104797 291502 928934 1291291 503592 469380 1087891 632012 871424 1314883 8733 60574 257917 1114726 2027 458281 161107 682575 1268398 768880 303151 526343 1324411 1016521 1164470 1315444 1221842 232586 1199310 417627 394034 247934 395334 601961 459279 1162139 1391929 1330883 1356524 899938 1306990 605310 746143 1308217 487317 1039388 191083 333111 1372955 852470 448861 912745 83904 54816 1086677 390453 1204441 1161314 748044 141944 1375051 49058 535592 379092 353598 358223 1072659 986145 588129 908764 1208401 1407696 1071024 91691 197820 937345 727660 15229 928041 313669 362975 758341 360214 402330 1391876 982337 1057282 420965 13058 898593 1188209 378065 281767 830118 1047308 366283 1200549 846244 316797 98486 1243145 1297700 843324 1292027 116822 1190595 565114 810053 191017 1385001 223965 167631 354975 1280227 297601 70219 967968 426951 557039 994371 595935 497098 979397 1011646 693944 178693 46047 141569 1254084 886801 316265 72550 657932 1204888 1245962 1064806 438185 1167328 1263049 1001830 10171 1326525 774294 811079 1063147 821192 227283 741143 247477 371067 883032 767050 924753 800327 109667 1394974 1108780 230671 777275 442791 543654 373925 824153 1377426 1157117 785543 1271694 380649 1387776 1162752 428021 754396 492386 97320 266752 352782 1052138 274200 364374 326296 802324 1265158 1256973 2858 790551 338291 1308762 991015 293909 191983 537302 293038 1187775 1392683 138985 446634 1417409 784154 184818 844759 1227570 633820 1293463 1130972 1357062 704269 258790 483339 1216431 1171044 906452 1307054 620759 720163 1348618 1197430 308950 1416640 400128 182465 212991 306199 1023406 1158760 1329654 49757 283555 1359658 368065 149047 487911 1302658 762782 681553 1345280 1377784 367142 1328023 269453 441430 902552 312563 795851 546369 359735 1334305 1280524 631142 784316 1019974 1158414 976963 846541 1209133 542740 406486 77760 234676 513462 1386553 168460 621079 912889 1413803 710953 553868 300644 31014 111365 1276167 643053 333738 334250 1393992 705784 1072110 956769 887512 649520 279996 252355 867399 170079 942930 765316 1049116 506313 299055 1076785 663111 579975 916900 1303979 202097 986255 315109 898448 773206 915627 776950 759074 338623 790308 773462 302603 19148 386707 726799 782956 628764 1322699 371464 230175 994206 491282 1025649 1077468 157991 376888 28993 774002 1325502 1128994 345127 836547 872711 572197 1196249 80488 382862 1353497 602988 1027495 886867 1110531 1139402 876817 392786 898474 620389 805227 324447 619716 303280 334983 726585 152563 1190851 234680 851010 1225324 1364305 830595 615514 1137957 1071726 890333 1413451 295207 519933 1354414 793041 425011 1393838 1143045 35251 1277362 1002974 822340 394726 1100226 858670 56907 337231 1128548 927357 542601 231593 270866 656425 368809 554547 642239 219557 696908 21252 1386978 954909 630345 535432 1173085 261131 1362014 214631 602805 753226 625020 640872 1169358 435978 417984 1242729 172173 210839 528256 111625 564565 1100496 977556 1250118 735564 316764 1073737 890409 1039759 1059961 1259252 107466 636466 1074086 502644 1201719 249698 1831 109302 1138702 1322562 500309 1303006 803731 1124102 697259 305538 386639 1168165 412126 90442 47427 357843 1298609 1064544 6440 1185811 141897 676683 1352129 167911 1393846 159136 1289728 897886 1207120 36586 629639 473166 1092094 1087979 667108 194128 532477 56894 765213 208166 608004 263979 1175442 666424 474831 151891 927297 901102 50356 765598 695096 384258 710664 1273001 1167242 946343 897985 1084342 845042 793433 1118178 1348353 471715 405074 522424 1318628 820155 1047574 1181448 544979 383607 1066152 974281 514567 1277494 995258 1126921 729009 6621 1251251 393322 267887 984742 796568 980877 40745 480250 772644 1392500 160221 1085424 858869 466642 1070501 390245 1410612 103379 570736 1101099 474768 263266 1251480 90807 702462 21556 982133 321350 675807 1371205 1197678 584779 520049 660277 1101356 864192 1201686 432423 1367097 204714 768002 899841 826569 1302289 315782 996376 1180774 1351244 1318666 838496 848974 743511 533576 1001321 1239156 371657 1315051 1021142 606705 544602 1267797 973867 1142347 1048508 593629 55095 150548 540517 1121485 865127 785079 838299 17691 109093 1407253 489648 786813 147313 1071413 534662 1123811 285151 917637 614768 140706 531028 1062156 425383 785131 1335648 1282405 617526 612043 323796 715654 11151 1364177 671344 675477 36582 1018242 1231838 817489 707577 1156017 954483 456531 10388 93830 1272757 610766 344554 976835 49404 1311518 26611 114076 101715 314013 1360537 810109 780225 1181884 240248 913693 1344122 1123745 526993 1219500 952654 1192231 1196463 322774 1242484 396888 700305 1077889 1341822 1207686 874543 1248538 1415555 1245767 748943 95694 1251962 902668 904913 501213 1174206 991062 1216919 706205 409556 1104471 91568 468498 734668 965798 421237 1136099 1400843 14554 295255 538578 146151 703414 283202 34301 48459 942755 1001335 1317994 539406 755875 37539 491357 1147608 460913 864080 1186314 1243415 484628 1091263 248534 338871 587984 597121 1013329 971306 802539 620734 492434 1339256 325409 885840 1224424 141560 475943 20085 379199 158901 350743 205368 1347571 1017776 1412196 684698 1067490 254007 1125688 683159 439867 153925 201592 489363 524521 1216814 1299176 1346492 461702 430427 1362774 150623 895766 4345 462143 1076814 1040984 1175724 262131 724546 1123180 795727 975640 921027 1231240 1337928 399151 992883 1320407 1014524 812189 973897 379345 311783 66261 645779 820308 1069681 1366863 388380 772676 1357406 824100 1054274 1285549 1264423 385716 1363316 642105 789457 162389 718440 972120 294485 1369604 254627 270369 1299984 834500 443650 940739 1350919 1416544 1325963 300346 783350 30579 613345 228309 331852 227532 540174 426437 1254135 470519 1066920 1044023 291663 944108 1330708 1047909 366962 1340451 1145991 884701 707512 692811 515509\n"
     ]
    }
   ],
   "source": [
    "knn = deck3[\"dists\"][:, 0].topk(2000)\n",
    "print(\" \".join(map(str, knn.indices.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ba9af6-1ed9-4736-ac42-83945b8acb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.02 11.84 10.90 10.74 10.59 9.97 9.97 9.97 9.14 7.39 7.16 6.89 6.76 6.65 6.63 6.58 6.56 6.49 6.44 6.44 6.35 6.32 6.27 6.27 6.24 6.19 6.17 6.12 6.09 6.02 5.84 5.81 5.67 5.62 5.61 5.60 5.58 5.58 5.56 5.53 5.52 5.52 5.51 5.51 5.50 5.48 5.47 5.44 5.43 5.43 5.42 5.40 5.39 5.39 5.39 5.36 5.35 5.30 5.30 5.27 5.27 5.25 5.22 5.22 5.18 5.16 5.16 5.16 5.15 5.15 5.15 5.14 5.13 5.11 5.11 5.09 5.09 5.09 5.09 5.08 5.08 5.05 5.05 5.04 5.03 5.03 5.03 5.03 5.02 5.02 5.02 5.00 5.00 5.00 5.00 4.99 4.99 4.98 4.98 4.98 4.98 4.97 4.97 4.97 4.97 4.96 4.95 4.95 4.95 4.95 4.94 4.93 4.93 4.93 4.92 4.92 4.91 4.90 4.89 4.88 4.88 4.86 4.86 4.84 4.83 4.83 4.82 4.82 4.82 4.81 4.80 4.80 4.79 4.78 4.78 4.77 4.76 4.75 4.74 4.74 4.73 4.72 4.72 4.72 4.72 4.70 4.70 4.70 4.69 4.69 4.68 4.67 4.67 4.67 4.66 4.66 4.66 4.64 4.63 4.63 4.63 4.61 4.60 4.58 4.57 4.57 4.56 4.56 4.54 4.54 4.53 4.53 4.50 4.48 4.47 4.46 4.45 4.42 4.42 4.42 4.41 4.41 4.40 4.40 4.38 4.38 4.37 4.36 4.36 4.34 4.34 4.32 4.32 4.30 4.30 4.29 4.29 4.28 4.28 4.27 4.27 4.26 4.25 4.24 4.24 4.24 4.24 4.24 4.24 4.23 4.23 4.23 4.22 4.22 4.22 4.21 4.21 4.21 4.20 4.20 4.19 4.19 4.19 4.18 4.17 4.17 4.16 4.15 4.15 4.12 4.11 4.11 4.10 4.10 4.10 4.10 4.09 4.09 4.08 4.07 4.06 4.04 4.04 4.04 4.04 4.02 4.02 4.02 4.02 4.01 4.01 4.00 3.99 3.99 3.98 3.98 3.98 3.97 3.95 3.94 3.94 3.93 3.92 3.91 3.90 3.90 3.90 3.90 3.88 3.88 3.88 3.88 3.87 3.87 3.86 3.85 3.84 3.84 3.84 3.83 3.83 3.78 3.78 3.78 3.78 3.77 3.77 3.77 3.77 3.77 3.76 3.75 3.75 3.75 3.75 3.75 3.74 3.72 3.72 3.72 3.71 3.71 3.69 3.69 3.65 3.64 3.64 3.64 3.64 3.61 3.61 3.59 3.59 3.59 3.58 3.58 3.58 3.58 3.58 3.55 3.55 3.55 3.54 3.54 3.51 3.46 3.46 3.45 3.45 3.45 3.43 3.43 3.43 3.42 3.41 3.40 3.39 3.37 3.36 3.36 3.36 3.35 3.35 3.34 3.32 3.32 3.31 3.31 3.30 3.30 3.27 3.26 3.23 3.20 3.20 3.18 3.14 3.12 3.12 3.11 3.11 3.04 3.04 3.04 2.97 2.97 2.97 2.95 2.94 2.87 2.87 2.87 2.83 2.81 2.80 2.77 2.76 2.75 2.69 2.69 2.68 2.68 2.67 2.66 2.66 2.66 2.64 2.62 2.59 2.57 2.54 2.52 2.51 2.50 2.46 2.42 2.42 2.42 2.42 2.41 2.40 2.39 2.35 2.35 2.35 2.32 2.32 2.32 2.28 2.27 2.26 2.26 2.24 2.22 2.20 2.20 2.20 2.20 2.19 2.19 2.16 2.15 2.15 2.15 2.15 2.14 2.14 2.13 2.13 2.11 2.11 2.11 2.10 2.07 2.05 2.05 2.04 2.03 2.01 2.00 1.99 1.95 1.94 1.93 1.92 1.92 1.92 1.92 1.92 1.90 1.88 1.85 1.85 1.85 1.85 1.85 1.85 1.85 1.85 1.82 1.82 1.82 1.82 1.82 1.82 1.80 1.78 1.78 1.77 1.77 1.77 1.77 1.77 1.77 1.77 1.77 1.76 1.76 1.76 1.76 1.75 1.75 1.74 1.74 1.73 1.72 1.71 1.71 1.71 1.71 1.71 1.70 1.69 1.69 1.69 1.67 1.67 1.67 1.66 1.66 1.66 1.66 1.65 1.64 1.64 1.64 1.64 1.64 1.64 1.63 1.63 1.63 1.62 1.62 1.62 1.62 1.62 1.62 1.61 1.59 1.58 1.54 1.54 1.54 1.53 1.51 1.50 1.50 1.49 1.47 1.47 1.47 1.46 1.46 1.46 1.45 1.45 1.44 1.44 1.44 1.43 1.43 1.42 1.42 1.42 1.42 1.42 1.41 1.41 1.40 1.40 1.40 1.39 1.39 1.39 1.38 1.38 1.37 1.36 1.35 1.35 1.34 1.34 1.34 1.34 1.34 1.34 1.34 1.34 1.33 1.33 1.33 1.33 1.32 1.31 1.30 1.28 1.28 1.26 1.26 1.25 1.25 1.25 1.25 1.25 1.25 1.25 1.24 1.23 1.22 1.21 1.19 1.18 1.18 1.18 1.18 1.18 1.16 1.15 1.15 1.15 1.14 1.14 1.13 1.13 1.13 1.13 1.12 1.11 1.11 1.11 1.10 1.10 1.10 1.10 1.10 1.10 1.09 1.08 1.07 1.07 1.07 1.06 1.06 1.06 1.06 1.06 1.06 1.06 1.05 1.05 1.05 1.05 1.04 1.04 1.04 1.04 1.03 1.03 1.03 1.02 1.02 1.01 1.01 1.01 1.01 1.01 1.01 1.00 1.00 1.00 1.00 1.00 1.00 1.00 0.99 0.99 0.99 0.98 0.98 0.97 0.96 0.96 0.96 0.96 0.96 0.96 0.96 0.96 0.96 0.95 0.95 0.95 0.95 0.95 0.94 0.94 0.94 0.94 0.93 0.93 0.93 0.93 0.93 0.93 0.93 0.93 0.93 0.92 0.91 0.91 0.91 0.91 0.91 0.91 0.90 0.90 0.90 0.89 0.89 0.89 0.89 0.89 0.88 0.88 0.88 0.88 0.88 0.88 0.87 0.86 0.86 0.86 0.86 0.86 0.86 0.86 0.85 0.84 0.84 0.84 0.84 0.83 0.83 0.83 0.83 0.83 0.82 0.82 0.82 0.82 0.81 0.81 0.81 0.80 0.80 0.80 0.80 0.80 0.80 0.79 0.79 0.79 0.79 0.78 0.78 0.78 0.78 0.77 0.77 0.77 0.77 0.77 0.76 0.76 0.76 0.74 0.74 0.74 0.73 0.73 0.73 0.73 0.73 0.72 0.72 0.72 0.72 0.71 0.71 0.71 0.71 0.71 0.71 0.71 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.69 0.69 0.69 0.69 0.69 0.69 0.69 0.68 0.68 0.68 0.68 0.68 0.68 0.68 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.65 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.63 0.63 0.63 0.63 0.63 0.63 0.63 0.63 0.63 0.62 0.62 0.62 0.62 0.62 0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.61 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.59 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.58 0.57 0.57 0.57 0.56 0.56 0.56 0.56 0.56 0.56 0.56 0.55 0.55 0.55 0.55 0.55 0.55 0.55 0.55 0.55 0.55 0.55 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.53 0.53 0.53 0.53 0.53 0.53 0.53 0.53 0.53 0.53 0.53 0.53 0.53 0.53 0.52 0.52 0.52 0.52 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.51 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.49 0.49 0.49 0.49 0.49 0.49 0.49 0.49 0.49 0.49 0.49 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.48 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.47 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.45 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.43 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.42 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.39 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.37 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.35 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.34 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.31 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.28 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.27 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24\n"
     ]
    }
   ],
   "source": [
    "knn = deck3[\"dists\"][:, 0].topk(2000)\n",
    "print(\" \".join(map(lambda x: f\"{x:.2f}\", knn.values.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d646da-0fb3-456a-acae-e868dc985083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.9702, 4.9781, 4.9800, 4.9847, 4.9921, 5.0372, 5.0385, 5.0829])\n",
      "tensor([339998, 227437, 240241, 328741, 202870,  10516, 110767,  17628])\n",
      "tensor(1)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "type=ANOM_ABEND msg=audit(1610091955.122:54204): auid=4294967295 uid=48 gid=48 ses=4294967295 subj=system_u:system_r:httpd_t:s0 pid=25413 comm=\"httpd\" reason=\"memory violation\" sig=11\n",
      "\n",
      "1 type=USER_AVC msg=audit(1613363937.137:2481): pid=813 uid=81 auid=4294967295 ses=4294967295 subj=system_u:system_r:system_dbusd_t:s0-s0:c0.c1023 msg='avc:  denied  { send_msg } for msgtype=method_call interface=org.freedesktop.login1.Manager member=CreateSession dest=org.freedesktop.login1 spid=30614 tpid=860 scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:systemd_logind_t:s0 tclass=dbus  exe=\"/usr/bin/dbus-daemon\" sauid=81 hostname=? addr=? terminal=?'\n",
      "\n",
      "1 type=AVC msg=audit(1608006885.884:140093): avc:  denied  { nlmsg_relay } for  pid=9856 comm=\"sudo\" scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=netlink_audit_socket permissive=1 type=AVC msg=audit(1608006885.884:140093): avc:  denied  { audit_write } for  pid=9856 comm=\"sudo\" capability=29  scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=capability permissive=1\n",
      "\n",
      "1 type=AVC msg=audit(1610086062.706:53355): avc:  denied  { nlmsg_relay } for  pid=20845 comm=\"sudo\" scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=netlink_audit_socket permissive=1 type=AVC msg=audit(1610086062.706:53355): avc:  denied  { audit_write } for  pid=20845 comm=\"sudo\" capability=29  scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=capability permissive=1\n",
      "\n",
      "1 type=AVC msg=audit(1608009738.459:141130): avc:  denied  { nlmsg_relay } for  pid=19533 comm=\"sudo\" scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=netlink_audit_socket permissive=1 type=AVC msg=audit(1608009738.459:141130): avc:  denied  { audit_write } for  pid=19533 comm=\"sudo\" capability=29  scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=capability permissive=1\n",
      "\n",
      "1 type=AVC msg=audit(1608006873.622:140080): avc:  denied  { nlmsg_relay } for  pid=9448 comm=\"sudo\" scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=netlink_audit_socket permissive=1 type=AVC msg=audit(1608006873.622:140080): avc:  denied  { audit_write } for  pid=9448 comm=\"sudo\" capability=29  scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=capability permissive=1\n",
      "\n",
      "1 type=USER_AVC msg=audit(1603070359.442:167): pid=820 uid=81 auid=4294967295 ses=4294967295 subj=system_u:system_r:system_dbusd_t:s0-s0:c0.c1023 msg='avc:  denied  { send_msg } for msgtype=method_return dest=:1.69 spid=815 tpid=7464 scontext=system_u:system_r:systemd_logind_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=dbus  exe=\"/usr/bin/dbus-daemon\" sauid=81 hostname=? addr=? terminal=?'\n",
      "\n",
      "1 type=USER_AVC msg=audit(1605233330.597:600): pid=831 uid=81 auid=4294967295 ses=4294967295 subj=system_u:system_r:system_dbusd_t:s0-s0:c0.c1023 msg='avc:  denied  { send_msg } for msgtype=method_return dest=:1.187 spid=890 tpid=10423 scontext=system_u:system_r:systemd_logind_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=dbus  exe=\"/usr/bin/dbus-daemon\" sauid=81 hostname=? addr=? terminal=?'\n",
      "\n",
      "1 type=AVC msg=audit(1600411630.262:5917): avc:  denied  { nlmsg_relay } for  pid=3314 comm=\"sudo\" scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=netlink_audit_socket permissive=1 type=AVC msg=audit(1600411630.262:5917): avc:  denied  { audit_write } for  pid=3314 comm=\"sudo\" capability=29  scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:system_r:httpd_t:s0 tclass=capability permissive=1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j = 538051\n",
    "print(deck3[\"dists\"][j])\n",
    "print(deck3[\"indices\"][j])\n",
    "print(deck3[\"fclevels\"][j])\n",
    "print(deck3[\"tlevels\"][j])\n",
    "print(df.full_log[j])\n",
    "\n",
    "print()\n",
    "for k in range(8):\n",
    "    kk = deck3[\"indices\"][j][k].item()\n",
    "    print(tdf.level[kk], tdf.full_log[kk])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97dc5d5a-8a63-4214-b36c-024126098d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver2\n",
    "def politic_draw(dists, indices, fclevel, tlevels):\n",
    "    dd = dists[-1] - dists[0]\n",
    "    dist = dists[0]\n",
    "    same = (tlevels == tlevels[0]).sum() == 8\n",
    "\n",
    "    # policy1: dist와 관계 없이 모든 tlevels가 3 또는 5이면 그 값을 출력\n",
    "    if same and tlevels[0] in [3, 5]:\n",
    "        return tlevels[0].item()\n",
    "\n",
    "    # policy2: dist와 관계 없이 모든 tlevels 중 앞의 4개가 2 또는 4 또는 6이면 그 값을 출력\n",
    "    if tlevels[0] in [2, 4, 6] and (tlevels[1:4] == tlevels[0]).sum() == 3:\n",
    "        return tlevels[0].item()\n",
    "\n",
    "    # policy: dist가 0.5보다 크면 level 7\n",
    "    if dist > 0.5:\n",
    "        return fclevel.item()\n",
    "\n",
    "    # 나머지\n",
    "    return tlevels[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79f4d8a0-8047-4e1b-9ffa-1919c7dfc616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver3\n",
    "def politic_draw(dists, indices, fclevel, tlevels):\n",
    "    dd = dists[-1] - dists[0]\n",
    "    dist = dists[0]\n",
    "    same = (tlevels == tlevels[0]).sum() == 8\n",
    "\n",
    "    # policy1: dist와 관계 없이 모든 tlevels가 3 또는 5이면 그 값을 출력\n",
    "    if same and tlevels[0] in [3, 5]:\n",
    "        return tlevels[0].item()\n",
    "\n",
    "    # policy2: dist와 관계 없이 모든 tlevels 중 앞의 4개가 2 또는 4 또는 6이면 그 값을 출력\n",
    "    if tlevels[0] in [2, 4, 6] and (tlevels[1:4] == tlevels[0]).sum() == 3:\n",
    "        return tlevels[0].item()\n",
    "\n",
    "    # policy: dist가 0.5보다 크면 level 7\n",
    "    if dist > 0.5:\n",
    "        return 7\n",
    "\n",
    "    # 나머지\n",
    "    return tlevels[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee473fc4-3bbe-4552-b514-0fe27be04941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 100\n",
    "politic_draw(deck3[\"dists\"][i], deck3[\"indices\"][i], deck3[\"fclevels\"][i], deck3[\"tlevels\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdefa37f-7fef-45c8-a6c3-88ae33cf88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1418916/1418916 [02:08<00:00, 11068.10it/s]\n"
     ]
    }
   ],
   "source": [
    "N = len(deck3[\"dists\"])\n",
    "outdic = {\"id\": list(range(1000000, 2418915 + 1)), \"level\": []}\n",
    "with tqdm(total=N, ncols=100, file=sys.stdout) as t:\n",
    "    for i in range(N):\n",
    "        v = politic_draw(deck3[\"dists\"][i], deck3[\"indices\"][i], deck3[\"fclevels\"][i], deck3[\"tlevels\"][i])\n",
    "        outdic[\"level\"].append(v)\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "103adf30-5447-4b60-9a07-428c3c1c635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf = pd.DataFrame(outdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18bb0afb-9b39-4f39-b408-e1671f9b3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdf.to_csv(\"results/squeezebert-uncased/squeezebert-uncased-focal-AdamW-lr1e-05_1-ver3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab0cdb-e01c-4d20-afde-deefd06449e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
