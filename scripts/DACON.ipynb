{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "searching-nightlife",
   "metadata": {},
   "source": [
    "# 수정사항\n",
    "\n",
    "**2021년 5월 15일 토요일 10:00**\n",
    "\n",
    "`MyTrainer.model`은 `nn.DataParallel`인데 후처리에서 그냥 `nn.Module`인 것 처럼 접근하고 있는 문제 수정했습니다.  \n",
    "`model.module.pre_classifier.register_forward_hook`으로 접근해야 하는데 `model.pre_classifier.register_forward_hook` 으로 접근하고 있었습니다.  \n",
    "\n",
    "# 머릿말\n",
    "\n",
    "※ 여러개의 `*.py` 또는 `*.ipynb` 파일들을 하나로 합치다보니 중간에 예기치못한 오류가 있을 수 있습니다. 댓글이나 이메일로 알려주시면 감사드립니다.  \n",
    "마찬가지로 모든 과정을 한번에 실행하려면 매우 많은 메모리와 연산이 필요합니다. 소스코드 중간중간 데이터를 저장하고 가로선으로 나누어진 부분이 있습니다. 이 부분을 기점으로 메모리를 초기화하며 진행했습니다.\n",
    "\n",
    "# 전체 요약\n",
    "\n",
    "## 1. 전처리\n",
    "\n",
    "**중복되는 데이터 제거**\n",
    "\n",
    "* 똑같은 텍스트를 갖은 데이터가 여럿 있습니다. 이 데이터들을 제거하면 train과 inference 시간이 약 2/3로 개선되었습니다.\n",
    "* 또한 텍스트는 똑같지만 level은 다른 데이터도 몇 있습니다. 중복되는 데이터 중에서 가장 많이 등장한 level만 남기고 전부 제거했습니다.\n",
    "\n",
    "**필요없는 텍스트 제거**\n",
    "\n",
    "* 날짜, 시간, PID, timestamp 등 필요없다고 생각되는 부분을 제거했습니다.\n",
    "\n",
    "**Oversampling**\n",
    "\n",
    "* level 2, 4, 6의 개수가 너무 적습니다. Train-validation을 나누고 나면 일부 특징은 해당 fold에서 아예 누락되어 학습을 못하기도 하기 때문에 level 2, 4, 6 인 데이터만 10배로 oversampling 해주었습니다. Oversampling할 때 특별히 augmenation을 적용하지는 않았고 그냥 복제를 했습니다.\n",
    "\n",
    "**기타**\n",
    "\n",
    "* token의 길이가 512를 초과하는 경우 앞의 512자리까지만 사용했습니다.\n",
    "* 원본 데이터 파일은 `./data/ori` 에 저장됩니다.\n",
    "* 데이터를 전처리해서 `pkl`파일으로 `./data/ver6` 에 저장했습니다.\n",
    "\n",
    "## 2. 학습\n",
    "\n",
    "* DistilBert를 finetune했고, FocalLoss를 썼습니다. 추가 데이터는 없습니다.\n",
    "* 5 fold cross validation을 했습니다. 하지만 결과를 합치기 전에 public score 0.9207, 합친 후에 0.9208으로 크게 차이는 없었습니다.\n",
    "\n",
    "## 3. 후처리\n",
    "\n",
    "* 모든 train 데이터의 feature를 저장해두고 test feature와 euclidean distance를 계산합니다. 이 거리 값을 level 추론에 사용합니다.\n",
    "\n",
    "## 4. 추론\n",
    "\n",
    "* \"기존에 나타난 log들과는 상이한 데이터가 level7지 않을까?\" 하는 가정으로 접근했습니다.\n",
    "* Level에 따라서 threshold를 각각 설정해주었고, threshold 또는 각종 조건을 넘어서면 level7, 아닐경우 fully connected layer의 출력과 distance를 종합해서 출력을 만들었습니다.\n",
    "\n",
    "## Environments\n",
    "\n",
    "* Ubuntu 18.04 LTS\n",
    "* RTX3090, cuda-toolkit 11.2\n",
    "* Checkpoint를 통해서 여러 장비들을 계속 옮겨가면서 작업했기 때문에 random seed 등의 문제로 완전한 reproduce는 어려울 수 있습니다.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* pytorch==1.7.1 # 이유는 모르겠지만 같은 weight를 써도 transformer 계열의 모델들은 1.7.1과 1.8.0 버전에서 전혀 다른 출력을 내더군요\n",
    "* numpy\n",
    "* pandas\n",
    "* matplotlib\n",
    "* pyaml\n",
    "* easydict\n",
    "* pytorch_transformers\n",
    "* sklearn\n",
    "* tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from pytorch_transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-institution",
   "metadata": {},
   "source": [
    "# 중복되는 데이터 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-quebec",
   "metadata": {},
   "source": [
    "## 중복되는 데이터 제거 - Train 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ori/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {}  # {텍스트: 레벨: 개수}\n",
    "with tqdm(total=len(df), ncols=100, file=sys.stdout) as t:\n",
    "    for i, (level, full_log) in enumerate(zip(df.level, df.full_log)):\n",
    "        text = full_log\n",
    "        if text not in db:\n",
    "            db[text] = {}\n",
    "        if level not in db[text]:\n",
    "            db[text][level] = {\"cnt\": 0, \"list\": []}\n",
    "        db[text][level][\"cnt\"] += 1\n",
    "        db[text][level][\"list\"].append(i)\n",
    "\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instrumental-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aquatic-formation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421079, 472972)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keys), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "competent-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "duples = {}\n",
    "for i, key in enumerate(keys):\n",
    "    if len(db[key]) != 1:\n",
    "        K = tuple(sorted(list(db[key].keys())))\n",
    "        if K not in duples:\n",
    "            duples[K] = []\n",
    "        duples[K].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confident-locator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0, 1), (0, 5), (3, 5), (0, 1, 5), (1, 5), (0, 3)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "terminal-freeze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 421079/421079 [00:00<00:00, 954321.56it/s]\n"
     ]
    }
   ],
   "source": [
    "outdb = {\"level\": [], \"text\": []}\n",
    "with tqdm(total=len(keys), ncols=100, file=sys.stdout) as t:\n",
    "    for key in keys:\n",
    "        d = db[key]\n",
    "        if len(list(d.keys())) > 1:\n",
    "            # 가장 개수가 많은 level을 선택\n",
    "            cnts = sorted([(d[k][\"cnt\"], k) for k in d.keys()], reverse=True)\n",
    "\n",
    "            # 만약 개수가 같은게 있다면 가장 level이 작은 것을 선택\n",
    "            max_cnt = cnts[0][0]\n",
    "            same_ks = list(filter(lambda x: x[0] == max_cnt, cnts))\n",
    "            level = same_ks[-1][1]\n",
    "        else:\n",
    "            level = list(d.keys())[0]\n",
    "\n",
    "        outdb[\"level\"].append(level)\n",
    "        outdb[\"text\"].append(key)\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "drawn-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = defaultdict(int)\n",
    "for level in outdb[\"level\"]:\n",
    "    temp[level] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "blind-hungary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {0: 280793, 1: 134195, 3: 4219, 5: 1842, 2: 12, 4: 10, 6: 8})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp  # 각 레벨별 데이터의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "formed-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/ver6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "separated-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle로 저장한다\n",
    "with open(\"data/ver6/train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(outdb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-parliament",
   "metadata": {},
   "source": [
    "## 중복되는 데이터 제거 - Test 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "premium-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ori/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "major-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_logs = [full_log for full_log in df.full_log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "timely-dividend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 1418916/1418916 [00:02<00:00, 609184.17it/s]\n"
     ]
    }
   ],
   "source": [
    "db = {}\n",
    "with tqdm(total=len(df), ncols=100, file=sys.stdout) as t:\n",
    "    for id, full_log in zip(df.id, df.full_log):\n",
    "        # text = MyDatasetVer5.refine_data(full_log)\n",
    "        text = full_log\n",
    "        if text not in db:\n",
    "            db[text] = []\n",
    "        db[text].append(id)\n",
    "\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "moderate-member",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1095951, 1418916)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.keys()), len(df)  # db: {full_log: \"list of ids\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "altered-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ver6/test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(db, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-southwest",
   "metadata": {},
   "source": [
    "## 중복되는 데이터 제거 - Level7 validation 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "norwegian-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ori/validation_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "widespread-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_logs = [full_log for full_log in df.full_log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "serious-scholarship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type=ANOM_PROMISCUOUS msg=audit(1600402733.466:4503): dev=enp2s0 prom=256 old_prom=0 auid=4294967295 uid=0 gid=0 ses=4294967295 type=SYSCALL msg=audit(1600402733.466:4503): arch=c000003e syscall=54 success=yes exit=0 a0=c a1=107 a2=1 a3=7f856aed1140 items=0 ppid=1 pid=12152 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=4294967295 comm=\"W#01-enp2s0\" exe=\"/usr/sbin/suricata\" subj=system_u:system_r:unconfined_service_t:s0 key=(null) type=PROCTITLE msg=audit(1600402733.466:4503): proctitle=2F7362696E2F7375726963617461002D63002F6574632F73757269636174612F73757269636174612E79616D6C002D2D70696466696C65002F7661722F72756E2F73757269636174612E706964002D6900656E70327330',\n",
       " 'oscap: msg: \"xccdf-result\", scan-id: \"0001600739632\", content: \"ssg-centos-7-ds.xml\", title: \"Prevent Log In to Accounts With Empty Password\", id: \"xccdf_org.ssgproject.content_rule_no_empty_passwords\", result: \"fail\", severity: \"high\", description: \"If an account is configured for password authentication but does not have an assigned password, it may be possible to log into the account without authentication. Remove any instances of the nullok option in /etc/pam.d/system-auth to prevent logins with empty passwords.\", rationale: \"If an account has an empty password, anyone could log in and run commands with the privileges of that account. Accounts with empty passwords should never be used in operational environments.\" references: \"AC-6 (http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf), IA-5(b) (http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf), IA-5(c) (http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf), IA-5(1)(a) (http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r4.pdf), 366 (http://iase.disa.mil/stigs/cci/Pages/index.aspx), SRG-OS-000480-GPOS-00227 (http://iase.disa.mil/stigs/srgs/Pages/index.aspx), Req-8.2.3 (https://www.pcisecuritystandards.org/documents/PCI_DSS_v3-1.pdf), 5.5.2 (https://www.fbi.gov/file-repository/cjis-security-policy-v5_5_20160601-2-1.pdf), 3.1.1 (http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-171.pdf), 3.1.5 (http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-171.pdf)\", identifiers: \"\", oval-id: \"oval:ssg-no_empty_passwords:def:1\", benchmark-id: \"xccdf_org.ssgproject.content_benchmark_RHEL-7\", profile-id: \"xccdf_org.ssgproject.content_profile_pci-dss\", profile-title: \"PCI-DSS v3 Control Baseline for Red Hat Enterprise Linux 7\".',\n",
       " 'Sep 22 10:56:19 localhost kernel: Out of memory: Kill process 1736 (probe_rpmverify) score 243 or sacrifice child']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bigger-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ver6/valid-level7.pkl\", \"wb\") as f:\n",
    "    pickle.dump(full_logs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6e160-b5e7-4181-91e8-226597a58930",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-straight",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735385ab-6c3b-4c40-b748-112e727ec73a",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-stanford",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    AverageMeter, referenced to https://dacon.io/competitions/official/235626/codeshare/1684\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if n > 0:\n",
    "            self.sum += val * n\n",
    "            self.cnt += n\n",
    "            self.avg = self.sum / self.cnt\n",
    "\n",
    "    def get(self):\n",
    "        return self.avg\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.avg\n",
    "\n",
    "    \n",
    "class CustomLogger:\n",
    "    def __init__(self, filename=None, filemode=\"a\", use_color=True):\n",
    "        if filename is not None:\n",
    "            self.empty = False\n",
    "            filename = Path(filename)\n",
    "            if filename.is_dir():\n",
    "                timestr = self._get_timestr().replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "                filename = filename / f\"log_{timestr}.log\"\n",
    "            self.file = open(filename, filemode)\n",
    "        else:\n",
    "            self.empty = True\n",
    "\n",
    "        self.use_color = use_color\n",
    "\n",
    "    def _get_timestr(self):\n",
    "        n = datetime.now()\n",
    "        return f\"{n.year:04d}-{n.month:02d}-{n.day:02d} {n.hour:02d}:{n.minute:02d}:{n.second:02d}\"\n",
    "\n",
    "    def _write(self, msg, level):\n",
    "        timestr = self._get_timestr()\n",
    "        out = f\"[{timestr} {level}] {msg}\"\n",
    "\n",
    "        if self.use_color:\n",
    "            if level == \" INFO\":\n",
    "                print(\"\\033[34m\" + out + \"\\033[0m\")\n",
    "            elif level == \" WARN\":\n",
    "                print(\"\\033[35m\" + out + \"\\033[0m\")\n",
    "            elif level == \"ERROR\":\n",
    "                print(\"\\033[31m\" + out + \"\\033[0m\")\n",
    "            elif level == \"FATAL\":\n",
    "                print(\"\\033[43m\\033[1m\" + out + \"\\033[0m\")\n",
    "            else:\n",
    "                print(out)\n",
    "        else:\n",
    "            print(out)\n",
    "\n",
    "        if not self.empty:\n",
    "            self.file.write(out + \"\\r\\n\")\n",
    "\n",
    "    def debug(self, *msg):\n",
    "        msg = \" \".join(map(str, msg))\n",
    "        self._write(msg, \"DEBUG\")\n",
    "\n",
    "    def info(self, *msg):\n",
    "        msg = \" \".join(map(str, msg))\n",
    "        self._write(msg, \" INFO\")\n",
    "\n",
    "    def warn(self, *msg):\n",
    "        msg = \" \".join(map(str, msg))\n",
    "        self._write(msg, \" WARN\")\n",
    "\n",
    "    def error(self, *msg):\n",
    "        msg = \" \".join(map(str, msg))\n",
    "        self._write(msg, \"ERROR\")\n",
    "\n",
    "    def fatal(self, *msg):\n",
    "        msg = \" \".join(map(str, msg))\n",
    "        self._write(msg, \"FATAL\")\n",
    "\n",
    "    def flush(self):\n",
    "        if not self.empty:\n",
    "            self.file.flush()\n",
    "    \n",
    "\n",
    "def seed_everything(seed, deterministic=False):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = deterministic\n",
    "        torch.backends.cudnn.benchmark = not deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa700de-5b59-4711-a595-f93046241902",
   "metadata": {},
   "source": [
    "## Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_word(text, deli=\" \"):\n",
    "    for i, t in enumerate(text):\n",
    "        if t == deli:\n",
    "            break\n",
    "    return text[:i]\n",
    "\n",
    "\n",
    "def remove_pattern(pattern, full_log):\n",
    "    for s in re.finditer(pattern, full_log):\n",
    "        a, b = s.span()\n",
    "        full_log = (full_log[:a] + full_log[b:]).strip()\n",
    "    return full_log\n",
    "\n",
    "\n",
    "def filttt(x):\n",
    "    if len(x) == 1:\n",
    "        return False\n",
    "\n",
    "    if re.fullmatch(r\"[\\.\\d,\\s-]+([ABTZ]|ms)?\", x):\n",
    "        return False\n",
    "\n",
    "    if x.lower() in (\"x64\", \"win32\", \"x86\", \"ko\", \"en\", \"kr\", \"us\", \"ko-kr\", \"en-us\"):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def refine_data(full_log):\n",
    "    t = first_word(full_log)\n",
    "    if len(t) == 4 and t.isdigit() and t[:2] in (\"19\", \"20\", \"21\"):\n",
    "        full_log = full_log[5:].strip()\n",
    "\n",
    "    t = first_word(full_log)\n",
    "    if len(t) == 3 and t in seasons:\n",
    "        full_log = full_log[4:].strip()\n",
    "\n",
    "        t = first_word(full_log)\n",
    "        if t.isdigit():\n",
    "            full_log = full_log[len(t) + 1 :].strip()\n",
    "\n",
    "    # 00:00:00 형식의 시간 이면?\n",
    "    if re.match(r\"\\d{2}:\\d{2}:\\d{2}\", full_log):\n",
    "        full_log = full_log[9:].strip()\n",
    "\n",
    "    if full_log.startswith(\"localhost\"):\n",
    "        full_log = full_log[10:].strip()\n",
    "\n",
    "    # @timestamp: \"~~~~Z\"\n",
    "    full_log = remove_pattern(r'\"@timestamp\"\\s?:\\s?\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z?\",?', full_log)\n",
    "    # \"pid\": \"4567\"\n",
    "    full_log = remove_pattern(r'\"pid\"\\s?:\\s?\\d+,?', full_log)\n",
    "    # [pid]\n",
    "    full_log = remove_pattern(r\"\\[\\d+\\]\", full_log)\n",
    "\n",
    "    full_log = re.sub(r\"\\s+\", \" \", full_log)\n",
    "\n",
    "    return full_log\n",
    "\n",
    "\n",
    "class MyDatasetVer7(Dataset):\n",
    "    \"\"\"\n",
    "    가장 결과가 좋았던 ver1을 따라간다\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, texts, levels=None) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.levels = levels\n",
    "        self.train = levels is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        text = refine_data(str(text))\n",
    "        otext = text  # backup original text before being tokenized\n",
    "\n",
    "        text = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        if len(text) < 512:  # padding\n",
    "            text = text + [0] * (512 - len(text))\n",
    "        elif len(text) > 512:  # cropping\n",
    "            text = text[:512]\n",
    "        text = torch.tensor(text, dtype=torch.long)\n",
    "\n",
    "        if self.train:  # train\n",
    "            level = self.levels[idx]\n",
    "            level = torch.tensor(level, dtype=torch.long)\n",
    "            return text, level, otext\n",
    "        else:  # test\n",
    "            return text, otext\n",
    "\n",
    "\n",
    "class MyDatasetVer7Test(Dataset):\n",
    "    def __init__(self, tokenizer, data) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.keys = list(self.data.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.keys[idx]\n",
    "        ids = self.data[text]\n",
    "        text = refine_data(text)\n",
    "        otext = text  # backup original text before being tokenized\n",
    "\n",
    "        text = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        if len(text) < 512:  # padding\n",
    "            text = text + [0] * (512 - len(text))\n",
    "        elif len(text) > 512:  # cropping\n",
    "            text = text[:512]\n",
    "        text = torch.tensor(text, dtype=torch.long)\n",
    "\n",
    "        return text, otext, ids\n",
    "\n",
    "\n",
    "class DatasetGeneratorVer7:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        seed,\n",
    "        fold,\n",
    "        tokenizer,\n",
    "        batch_size,\n",
    "        num_workers,\n",
    "        train_shuffle=True,\n",
    "        oversampling=False,\n",
    "        oversampling_scale=50,\n",
    "    ):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.seed = seed\n",
    "        self.fold = fold\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_shuffle = train_shuffle\n",
    "        self.oversampling = oversampling\n",
    "        self.oversampling_scale = oversampling_scale\n",
    "\n",
    "        self.dl_kwargs = dict(batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def train_valid(self):\n",
    "        # train dataset\n",
    "        with open(self.data_dir / \"train.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        levels = np.array(data[\"level\"], dtype=np.long)\n",
    "        texts = np.array(data[\"text\"], dtype=np.object)\n",
    "\n",
    "        if self.oversampling:\n",
    "            # oversampling -- level 2\n",
    "            mask = levels == 2\n",
    "            o2_len = len(levels[mask])\n",
    "            o2_levels = np.array([2] * o2_len * self.oversampling_scale)\n",
    "            o2_texts = np.concatenate([texts[mask] for _ in range(self.oversampling_scale)])\n",
    "\n",
    "            # oversampling -- level 4\n",
    "            mask = levels == 4\n",
    "            o4_len = len(levels[mask])\n",
    "            o4_levels = np.array([4] * o4_len * self.oversampling_scale)\n",
    "            o4_texts = np.concatenate([texts[mask] for _ in range(self.oversampling_scale)])\n",
    "\n",
    "            # oversampling -- level 6\n",
    "            mask = levels == 6\n",
    "            o6_len = len(levels[mask])\n",
    "            o6_levels = np.array([6] * o6_len * self.oversampling_scale)\n",
    "            o6_texts = np.concatenate([texts[mask] for _ in range(self.oversampling_scale)])\n",
    "\n",
    "            levels = np.concatenate([levels, o2_levels, o4_levels, o6_levels])\n",
    "            texts = np.concatenate([texts, o2_texts, o4_texts, o6_texts])\n",
    "\n",
    "        # k-fold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=self.train_shuffle, random_state=self.seed)\n",
    "        indices = list(skf.split(texts, levels))\n",
    "        tidx, vidx = indices[self.fold - 1]\n",
    "        tds = MyDatasetVer7(self.tokenizer, texts[tidx], levels[tidx])\n",
    "        vds = MyDatasetVer7(self.tokenizer, texts[vidx], levels[vidx])\n",
    "\n",
    "        tdl = DataLoader(tds, shuffle=self.train_shuffle, **self.dl_kwargs)\n",
    "        vdl = DataLoader(vds, shuffle=False, **self.dl_kwargs)\n",
    "        return tdl, vdl\n",
    "\n",
    "    def train_only(self):\n",
    "        with open(self.data_dir / \"train.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        levels = np.array(data[\"level\"], dtype=np.long)\n",
    "        texts = np.array(data[\"text\"], dtype=np.object)\n",
    "\n",
    "        ds = MyDatasetVer7(self.tokenizer, texts, levels)\n",
    "        dl = DataLoader(ds, shuffle=False, **self.dl_kwargs)\n",
    "        return dl\n",
    "\n",
    "    def valid_lv7(self):\n",
    "        # validation level 7 dataset\n",
    "        with open(self.data_dir / \"valid-level7.pkl\", \"rb\") as f:\n",
    "            texts = pickle.load(f)\n",
    "\n",
    "        ds = MyDatasetVer7(self.tokenizer, texts)\n",
    "        dl = DataLoader(ds, shuffle=False, **self.dl_kwargs)\n",
    "        return dl\n",
    "\n",
    "    def test(self):\n",
    "        # test dataset\n",
    "        with open(self.data_dir / \"test.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        ds = MyDatasetVer7Test(self.tokenizer, data)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d142733-8026-48b8-bf01-4d4de2af85f4",
   "metadata": {},
   "source": [
    "## Trainer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    https://dacon.io/competitions/official/235585/codeshare/1796\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        # print(self.gamma)\n",
    "        self.eps = eps\n",
    "        self.ce = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-musical",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyTrainer:\n",
    "    _tqdm_ = dict(ncols=100, leave=False, file=sys.stdout)\n",
    "\n",
    "    def __init__(self, config, fold, checkpoint=None) -> None:\n",
    "        self.C = config\n",
    "        self.fold = fold\n",
    "\n",
    "        # model\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(self.C.model.name)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(self.C.model.name, num_labels=7).cuda()\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "        # loss\n",
    "        self.criterion = FocalLoss(self.C.train.loss.params.gamma).cuda()\n",
    "        # optimizer\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.C.train.lr)\n",
    "        # scheduler\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, **self.C.train.scheduler.params)\n",
    "\n",
    "        self.epoch = 1\n",
    "        self.best_loss = math.inf\n",
    "        self.best_acc = 0.0\n",
    "        self.earlystop_cnt = 0\n",
    "        self._freeze_step = 3\n",
    "\n",
    "        if checkpoint is not None:\n",
    "            if Path(checkpoint).exists():\n",
    "                self.load(checkpoint)\n",
    "            else:\n",
    "                self.C.log.info(\"No checkpoint file\", checkpoint)\n",
    "\n",
    "        # dataset\n",
    "        self.dsgen = DatasetGeneratorVer8(self.C, self.tokenizer, shuffle=True)\n",
    "        self.tdl, self.vdl = self.dsgen.train_valid(self.fold)\n",
    "\n",
    "    def _freeze_step1(self):\n",
    "        self._freeze_step = 1\n",
    "        self.model.module.requires_grad_(False)\n",
    "        self.model.module.classifier.requires_grad_(True)\n",
    "\n",
    "    def _freeze_step2(self):\n",
    "        self._freeze_step = 2\n",
    "        self.model.module.requires_grad_(True)\n",
    "        self.model.module.classifier.requires_grad_(False)\n",
    "\n",
    "    def _freeze_step3(self):\n",
    "        self._freeze_step = 3\n",
    "        self.model.module.requires_grad_(True)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": self.model.module.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "                \"epoch\": self.epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"best_acc\": self.best_acc,\n",
    "                \"earlystop_cnt\": self.earlystop_cnt,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load pretrained\", path)\n",
    "        ckpt = torch.load(path)\n",
    "        self.model.module.load_state_dict(ckpt[\"model\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        self.epoch = ckpt[\"epoch\"] + 1\n",
    "        self.best_loss = ckpt[\"best_loss\"]\n",
    "        self.best_acc = ckpt[\"best_acc\"]\n",
    "        self.earlystop_cnt = ckpt[\"earlystop_cnt\"]\n",
    "\n",
    "    def train_loop(self):\n",
    "        self.model.train()\n",
    "\n",
    "        O = MyOutput()\n",
    "        with tqdm(total=len(self.tdl.dataset), desc=f\"Train {self.epoch:03d}\", **self._tqdm_) as t:\n",
    "            for text, tlevel, otext in self.tdl:\n",
    "                text_ = text.cuda()\n",
    "                tlevel_ = tlevel.cuda()\n",
    "                plevel_ = self.model(text_)[0]\n",
    "                loss = self.criterion(plevel_, tlevel_)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                pvlevel_ = plevel_.detach().argmax(dim=1)\n",
    "                acc = (pvlevel_ == tlevel_).sum() / len(text) * 100\n",
    "                O.loss.update(loss.item(), len(text))\n",
    "                O.acc.update(acc.item(), len(text))\n",
    "                O.plevels.append(pvlevel_.cpu())\n",
    "                O.tlevels.append(tlevel)\n",
    "                t.set_postfix_str(f\"loss: {O.loss():.6f}, acc: {O.acc():.2f}\", refresh=False)\n",
    "                t.update(len(text))\n",
    "        return O.freeze()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_loop(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        O = MyOutput()\n",
    "        with tqdm(total=len(self.vdl.dataset), desc=f\"Valid {self.epoch:03d}\", **self._tqdm_) as t:\n",
    "            for text, tlevel, otext in self.vdl:\n",
    "                text_ = text.cuda()\n",
    "                tlevel_ = tlevel.cuda()\n",
    "                plevel_ = self.model(text_)[0]\n",
    "                loss = self.criterion(plevel_, tlevel_)\n",
    "\n",
    "                pvlevel_ = plevel_.detach().argmax(dim=1)\n",
    "                acc = (pvlevel_ == tlevel_).sum() / len(text) * 100\n",
    "                O.loss.update(loss.item(), len(text))\n",
    "                O.acc.update(acc.item(), len(text))\n",
    "                O.plevels.append(pvlevel_.cpu())\n",
    "                O.tlevels.append(tlevel)\n",
    "                t.set_postfix_str(f\"loss: {O.loss():.6f}, acc: {O.acc():.2f}\", refresh=False)\n",
    "                t.update(len(text))\n",
    "        return O.freeze()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def callback(self, to: MyOutput, vo: MyOutput):\n",
    "        # f1 score\n",
    "        tf1 = f1_score(to.tlevels, to.plevels, zero_division=1, average=\"macro\")\n",
    "        vf1 = f1_score(vo.tlevels, vo.plevels, zero_division=1, average=\"macro\")\n",
    "        trep = str(classification_report(to.tlevels, to.plevels, labels=[0, 1, 2, 3, 4, 5, 6], zero_division=1))\n",
    "        vrep = str(classification_report(vo.tlevels, vo.plevels, labels=[0, 1, 2, 3, 4, 5, 6], zero_division=1))\n",
    "\n",
    "        self.C.log.info(\n",
    "            f\"Epoch: {self.epoch:03d}/{self.C.train.max_epochs},\",\n",
    "            f\"loss: {to.loss:.6f};{vo.loss:.6f},\",\n",
    "            f\"acc {to.acc:.2f};{vo.acc:.2f}\",\n",
    "            f\"f1 {tf1:.2f}:{vf1:.2f}\",\n",
    "        )\n",
    "        self.C.log.info(\"Train Report\\r\\n\" + trep)\n",
    "        self.C.log.info(\"Validation Report\\r\\n\" + vrep)\n",
    "        self.C.log.flush()\n",
    "\n",
    "        if isinstance(self.scheduler, ReduceLROnPlateau):\n",
    "            self.scheduler.step(vo.loss)\n",
    "\n",
    "        if self.best_loss - vo.loss > 1e-6 or vf1 - self.best_acc > 1e-6:\n",
    "            if self.best_loss > vo.loss:\n",
    "                self.best_loss = vo.loss\n",
    "            else:\n",
    "                self.best_acc = vf1\n",
    "\n",
    "            self.earlystop_cnt = 0\n",
    "            self.save(self.C.result_dir / f\"{self.C.uid}_{self.fold}.pth\")\n",
    "\n",
    "            # TODO 결과 요약 이미지 출력\n",
    "        else:\n",
    "            self.earlystop_cnt += 1\n",
    "\n",
    "    def fit(self):\n",
    "        for self.epoch in range(self.epoch, self.C.train.max_epochs + 1):\n",
    "            if self.C.train.finetune.do:\n",
    "                if self.epoch <= self.C.train.finetune.step1_epochs:\n",
    "                    if self._freeze_step != 1:\n",
    "                        self.C.log.info(\"Finetune Step 1\")\n",
    "                        self._freeze_step1()\n",
    "                elif self.epoch <= self.C.train.finetune.step2_epochs:\n",
    "                    if self._freeze_step != 2:\n",
    "                        self.C.log.info(\"Finetune Step 2\")\n",
    "                        self._freeze_step2()\n",
    "                elif self.epoch > self.C.train.finetune.step2_epochs:\n",
    "                    if self._freeze_step != 3:\n",
    "                        self.C.log.info(\"Finetune Step 3\")\n",
    "                        self._freeze_step3()\n",
    "\n",
    "            to = self.train_loop()\n",
    "            vo = self.valid_loop()\n",
    "            self.callback(to, vo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3ffd6-98b9-4968-b31b-ca2786bb76ba",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-colors",
   "metadata": {},
   "source": [
    "`./config/distilbert-base-uncased-ver7.yaml` 파일의 내용입니다.\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  name: distilbert-base-uncased\n",
    "comment: null # 기타 추가할 메모\n",
    "result_dir: results/distilbert-base-uncased-ver7 # 출력 파일들이 저장될 경로 (*.log 로그 파일과, *.pth 체크포인트가 저장됩니다.)\n",
    "\n",
    "debug: false\n",
    "seed: 20210425\n",
    "ver: 7\n",
    "\n",
    "train:\n",
    "  SAM: false\n",
    "  folds: \n",
    "    - 1\n",
    "    - 2\n",
    "    - 3  # 몇번 째 fold들을 학습할건지. 1~5 사이의 값.\n",
    "    - 4  # 한 fold마다 GPU에 따라 15~25시간 정도 걸립니다.\n",
    "    - 5\n",
    "  checkpoints: \n",
    "    - null\n",
    "    - null\n",
    "    - null  # 학습을 checkpoint부터 다시 시작할 때 설정\n",
    "    - null\n",
    "    - null\n",
    "  loss: \n",
    "    name: focal # ce, focal, arcface\n",
    "    params:\n",
    "      gamma: 2.0\n",
    "      s: 45.0\n",
    "      m: 0.1\n",
    "      crit: focal\n",
    "    \n",
    "  optimizer:\n",
    "    name: AdamW # Adam, AdamW\n",
    "  \n",
    "  finetune:\n",
    "    do: true  # tail부분만 2epochs, body만 2epochs, 전체 8epochs\n",
    "    step1_epochs: 2\n",
    "    step2_epochs: 4\n",
    "  max_epochs: 12\n",
    "    \n",
    "  lr: 0.00001\n",
    "  scheduler:\n",
    "    name: ReduceLROnPlateau\n",
    "    params:\n",
    "      factor: 0.5\n",
    "      patience: 3\n",
    "      verbose: true\n",
    "  \n",
    "dataset:\n",
    "  dir: data/ver6  # 데이터셋 경로\n",
    "  batch_size: 35  # Batch size 35에 약 22GB 정도의 GPU 메모리가 필요(finetune-step3 기준)\n",
    "  num_workers: 8\n",
    "  oversampling: true  # level 2, 4, 6에 대해서 10배로 데이터를 복제해줍니다.\n",
    "  oversampling_scale: 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    CONFIG_PATH = \"config/distilbert-base-uncased-ver7.yaml\"\n",
    "    \n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "        C = EasyDict(yaml.load(f, yaml.FullLoader))\n",
    "\n",
    "    for fold, checkpoint in zip(C.train.folds, C.train.checkpoints):\n",
    "        with open(CONFIG_PATH, \"r\") as f:\n",
    "            C = EasyDict(yaml.load(f, yaml.FullLoader))\n",
    "            Path(C.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if C.dataset.num_workers < 0:\n",
    "                C.dataset.num_workers = multiprocessing.cpu_count()\n",
    "            C.uid = f\"{C.model.name.split('/')[-1]}-{C.train.loss.name}\"\n",
    "            C.uid += f\"-{C.train.optimizer.name}\"\n",
    "            C.uid += f\"-lr{C.train.lr}\"\n",
    "            C.uid += f\"-ver{C.ver}\" if C.ver > 1 else \"\"\n",
    "            C.uid += f\"-os{C.dataset.oversampling_scale}\" if C.dataset.oversampling else \"\"\n",
    "            C.uid += \"-sam\" if C.train.SAM else \"\"\n",
    "            C.uid += f\"-{C.comment}\" if C.comment is not None else \"\"\n",
    "            print(C.uid)\n",
    "\n",
    "            log = CustomLogger(Path(C.result_dir) / f\"{C.uid}_{fold}.log\", \"a\")\n",
    "            log.info(\"\\r\\n\" + pformat(C))\n",
    "            log.flush()\n",
    "\n",
    "            C.log = log\n",
    "            C.result_dir = Path(C.result_dir)\n",
    "            C.dataset.dir = Path(C.dataset.dir)\n",
    "            seed_everything(C.seed, deterministic=False)\n",
    "\n",
    "        C.log.info(\"Fold\", fold, \", checkpoint\", checkpoint)\n",
    "        trainer = MyTrainer(C, fold, checkpoint)\n",
    "        trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-gilbert",
   "metadata": {},
   "source": [
    "**학습 로그**\n",
    "\n",
    "```log\n",
    "[2021-05-11 19:04:32  INFO] Epoch: 006/12, loss: 0.001347;0.001057, acc 99.90;99.91 f1 0.93:0.99\n",
    "[2021-05-11 19:04:32  INFO] Train Report\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00    224634\n",
    "           1       1.00      1.00      1.00    107356\n",
    "           2       0.94      0.87      0.90       106\n",
    "           3       1.00      1.00      1.00      3376\n",
    "           4       0.82      0.98      0.89        88\n",
    "           5       1.00      0.98      0.99      1473\n",
    "           6       0.90      0.64      0.75        70\n",
    "\n",
    "    accuracy                           1.00    337103\n",
    "   macro avg       0.95      0.92      0.93    337103\n",
    "weighted avg       1.00      1.00      1.00    337103\n",
    "\n",
    "[2021-05-11 19:04:32  INFO] Validation Report\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     56159\n",
    "           1       1.00      1.00      1.00     26839\n",
    "           2       1.00      0.96      0.98        26\n",
    "           3       1.00      0.99      1.00       843\n",
    "           4       1.00      1.00      1.00        22\n",
    "           5       1.00      0.99      0.99       369\n",
    "           6       0.95      1.00      0.97        18\n",
    "\n",
    "    accuracy                           1.00     84276\n",
    "   macro avg       0.99      0.99      0.99     84276\n",
    "weighted avg       1.00      1.00      1.00     84276\n",
    "\n",
    "<기타 생략>\n",
    "\n",
    "[2021-05-12 01:29:26  INFO] Epoch: 012/12, loss: 0.000648;0.001045, acc 99.92;99.90 f1 0.99:1.00\n",
    "[2021-05-12 01:29:26  INFO] Train Report\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00    224634\n",
    "           1       1.00      1.00      1.00    107356\n",
    "           2       0.96      0.98      0.97       106\n",
    "           3       1.00      1.00      1.00      3376\n",
    "           4       1.00      1.00      1.00        88\n",
    "           5       1.00      0.99      0.99      1473\n",
    "           6       1.00      1.00      1.00        70\n",
    "\n",
    "    accuracy                           1.00    337103\n",
    "   macro avg       0.99      1.00      0.99    337103\n",
    "weighted avg       1.00      1.00      1.00    337103\n",
    "\n",
    "[2021-05-12 01:29:26  INFO] Validation Report\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     56159\n",
    "           1       1.00      1.00      1.00     26839\n",
    "           2       1.00      1.00      1.00        26\n",
    "           3       1.00      1.00      1.00       843\n",
    "           4       1.00      1.00      1.00        22\n",
    "           5       0.99      0.99      0.99       369\n",
    "           6       1.00      1.00      1.00        18\n",
    "\n",
    "    accuracy                           1.00     84276\n",
    "   macro avg       1.00      1.00      1.00     84276\n",
    "weighted avg       1.00      1.00      1.00     84276\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db68b5c-e2d2-47f1-ae27-364ce73980c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-print",
   "metadata": {},
   "source": [
    "# 후처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-iceland",
   "metadata": {},
   "source": [
    "모든 Train 데이터에 대한 feature map을 구하고 저장합니다. 이 feature map의 모음을 여기서는 \"deck\"이라고 부르겠습니다.\n",
    "\n",
    "이 부분은 학습과는 별개로 진행되었기 때문에 메모리를 초기화하고 config 읽기와 trainer 생성부터 다시 진행됩니다.  \n",
    "이 부분은 fold 3에 대해서만 작성했습니다. Cross validation을 하려면 같은 방법으로 5개의 fold에 대해 총 5회 반복해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "threaded-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix = \"distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3\"\n",
    "outdir = Path(\"results/distilbert-base-uncased-ver7\")\n",
    "fold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respected-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/distilbert-base-uncased-ver7.yaml\", \"r\") as f:\n",
    "    C = EasyDict(yaml.load(f, yaml.FullLoader))\n",
    "    C.result_dir = Path(C.result_dir)\n",
    "    C.dataset.dir = Path(C.dataset.dir)\n",
    "    seed_everything(C.seed, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ordinary-absolute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3.pth\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(C, fold, outdir / f\"{postfix}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compliant-remark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fdcd9cafe10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer.model\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "frozen-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = []\n",
    "\n",
    "\n",
    "def hook(model, input, output):\n",
    "    activation.append(output.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "theoretical-tuner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fdb7a876c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.pre_classifier.register_forward_hook(hook)  # set feature hook function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comparable-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = trainer.dsgen.train_only()  # shuffle 없는 모든 train 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-adapter",
   "metadata": {},
   "source": [
    "### Make Train Deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-hours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████▉             | 319130/421079 [15:50<05:03, 336.34it/s]"
     ]
    }
   ],
   "source": [
    "activation = []\n",
    "deck = {\"feat\": [], \"otext\": [], \"tlevel\": [], \"fclevel\": []}\n",
    "with tqdm(total=len(train_dl.dataset), ncols=100, file=sys.stdout) as t:\n",
    "    for text, tlevel, otext in train_dl:\n",
    "        fclevel = model(text.cuda(non_blocking=True))[0].argmax(dim=1).cpu()\n",
    "        deck[\"fclevel\"].append(fclevel)\n",
    "        deck[\"tlevel\"].append(tlevel)\n",
    "        deck[\"otext\"].extend(otext)\n",
    "        t.update(text.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"feat\"] = torch.cat(activation)\n",
    "deck[\"tlevel\"] = torch.cat(deck[\"tlevel\"])\n",
    "deck[\"fclevel\"] = torch.cat(deck[\"fclevel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"tlevel\"].shape, deck[\"fclevel\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(deck, outdir / f\"{postfix}-deck1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c5cd1-e7fc-450d-9889-cbfd4bcd5574",
   "metadata": {},
   "source": [
    "### Valid Lv7에 대한 결과 분석\n",
    "\n",
    "가장 거리가 가까운 4개의 거리, 인덱스, level을 출력해봅니다.\n",
    "\n",
    "메모리가 20GB이상이 아니면 out of memory error 가 발생할 수 있습니다.  \n",
    "메모리가 부족하면 테스트 데이터를 쪼개서 해보는 것도 좋을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f224b805-2318-40c3-9369-05a6362bdec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lv7 = trainer.dsgen.valid_lv7().dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a09fee9-53ab-403e-a2bc-e7f26f69e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.8830,  4.1110, -5.7687, -4.4678, -7.3731, -4.0645, -6.7607]])\n"
     ]
    }
   ],
   "source": [
    "text, otext = ds_lv7[0]\n",
    "activation = []\n",
    "print(model(text[None].cuda())[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05fa6bc5-d0bc-4435-b20e-d32328ff1aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24.6197, 32.4965, 36.9424, 40.4677]]),\n",
       " tensor([[144, 177, 112, 195]]),\n",
       " tensor([[0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists, indices = torch.norm(deck[\"feat\"] - activation[0][None], p=None, dim=1).topk(4, largest=False)\n",
    "dists, indices, deck[\"tlevel\"][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "897ffbfe-9af4-416d-914e-b56ed9f73ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2189, -2.4237, -7.7600,  1.8829, -8.6021, -5.1994, -7.5476]])\n"
     ]
    }
   ],
   "source": [
    "text, otext = ds_lv7[1]\n",
    "activation = []\n",
    "print(model(text[None].cuda())[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bba0a365-2c01-46f9-b140-9da948003f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[38.2679, 40.8461, 46.9950, 47.6562]]),\n",
       " tensor([[536, 236, 272, 377]]),\n",
       " tensor([[0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists, indices = torch.norm(deck[\"feat\"] - activation[0][None], p=None, dim=1).topk(4, largest=False)\n",
    "dists, indices, deck[\"tlevel\"][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59c09a7f-ee94-4e2d-b2ce-4875e3ac43d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8619, -2.2384, -1.8922, -2.0378, -3.1984, -2.5976, -2.0431]])\n"
     ]
    }
   ],
   "source": [
    "text, otext = ds_lv7[2]\n",
    "activation = []\n",
    "print(model(text[None].cuda())[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9a70679-c882-4e14-bace-4ce444e5aea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[29.5083, 43.1404, 52.9118, 53.9867]]),\n",
       " tensor([[144, 103,  82, 757]]),\n",
       " tensor([[0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists, indices = torch.norm(deck[\"feat\"] - activation[0][None], p=None, dim=1).topk(4, largest=False)\n",
    "dists, indices, deck[\"tlevel\"][indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30353f-26d0-47b6-ba00-bb3d1260eae5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-afghanistan",
   "metadata": {},
   "source": [
    "### Make Test Deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58d4ff2-76d5-46c6-94a6-3242ba3aee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix = \"distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3\"\n",
    "outdir = Path(\"results/distilbert-base-uncased-ver7\")\n",
    "fold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f9b15f-5a8a-4319-a621-eaea83bdd73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/distilbert-base-uncased-ver7.yaml\", \"r\") as f:\n",
    "    C = EasyDict(yaml.load(f, yaml.FullLoader))\n",
    "    C.result_dir = Path(C.result_dir)\n",
    "    C.dataset.dir = Path(C.dataset.dir)\n",
    "    seed_everything(C.seed, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7b4506-3a82-48f9-b56c-0e0676f1efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3.pth\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(C, fold, outdir / f\"{postfix}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb53cf4-7685-41e7-9f20-6b44637fb727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fdcd9cafe10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = trainer.model\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53f0531-b82c-4fe2-9086-b65818304854",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = []\n",
    "\n",
    "\n",
    "def hook(model, input, output):\n",
    "    activation.append(output.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d7141f-a645-4c41-a79a-01a7a8e902f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fdb7a876c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.pre_classifier.register_forward_hook(hook)  # set feature hook function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "capable-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = trainer.dsgen.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "crucial-arrest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1095951/1095951 [1:42:37<00:00, 177.99it/s]\n"
     ]
    }
   ],
   "source": [
    "activation = []\n",
    "deck = {\"feat\": [], \"otext\": [], \"fclevel\": [], \"ids\": []}\n",
    "with tqdm(total=len(ds_test), ncols=100, file=sys.stdout) as t:\n",
    "    for i in range(len(ds_test)):\n",
    "        text, otext, ids = ds_test[i]\n",
    "        fclevel = model(text[None].cuda(non_blocking=True))[0].argmax(dim=1).cpu()\n",
    "        deck[\"fclevel\"].append(fclevel)\n",
    "        deck[\"otext\"].append(otext)\n",
    "        deck[\"ids\"].append(ids)\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"feat\"] = torch.stack(activation)\n",
    "deck[\"fclevel\"] = torch.stack(deck[\"fclevel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "global-compromise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1095951, 1, 768]), torch.Size([1095951]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deck[\"feat\"].shape, deck[\"fclevel\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "limited-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck[\"feat\"] = deck[\"feat\"][:, 0, :]\n",
    "# deck[\"fclevel\"] = deck[\"fclevel\"][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "operating-serbia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1095951, 768]), torch.Size([1095951]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deck[\"feat\"].shape, deck[\"fclevel\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "hungry-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(deck, outdir / f\"{postfix}-deck2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50cf5cf-8ecd-45f6-ada1-d3cc0d03ccc4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-horizontal",
   "metadata": {},
   "source": [
    "## 모든 train feature에 대한 test feature의 거리 구하기\n",
    "\n",
    "이 부분은 위와는 별개로 진행되었기 때문에 저장된 deck을 다시 로딩합니다. (한 번에 하려면 메모리가 부족합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fundamental-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck1 = torch.load(\"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3-deck1.pth\")\n",
    "deck2 = torch.load(\"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3-deck2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "guided-minute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([421079, 768]), torch.Size([1095951, 768]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deck1[\"feat\"].shape, deck2[\"feat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "knowing-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck1[\"feat_\"] = deck1[\"feat\"].cuda()\n",
    "deck2[\"feat_\"] = deck2[\"feat\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "light-trial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1095951/1095951 [2:05:02<00:00, 146.07it/s]\n"
     ]
    }
   ],
   "source": [
    "N = deck2[\"feat\"].size(0)\n",
    "distdeck = {\"dist\": [], \"level\": []}\n",
    "with tqdm(total=N, ncols=100, file=sys.stdout) as t:\n",
    "    for i in range(N):\n",
    "        dist_ = torch.norm(deck1[\"feat_\"] - deck2[\"feat_\"][i, None], p=None, dim=1)\n",
    "        dist_, indices_ = dist_.topk(4, largest=False)\n",
    "        tlevels = deck1[\"tlevel\"][indices_]\n",
    "\n",
    "        distdeck[\"dist\"].append(dist_.cpu())\n",
    "        distdeck[\"level\"].append(tlevels)\n",
    "\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "postal-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "distdeck[\"dist\"] = torch.stack(distdeck[\"dist\"])\n",
    "distdeck[\"level\"] = torch.stack(distdeck[\"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "electronic-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    distdeck, \"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3-distdeck.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3a946-d756-4ccd-947c-d565a63807f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c62aae-37fc-4c64-ab04-c6db4f66972f",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-conspiracy",
   "metadata": {},
   "source": [
    "## Submission 파일 만들기\n",
    "\n",
    "이 부분은 위와는 별개로 진행되었기 때문에 저장된 deck을 다시 로딩합니다. (한 번에 하려면 메모리가 부족합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chinese-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "distdeck = torch.load(\n",
    "    \"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3-distdeck.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lesbian-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "deck2 = torch.load(\"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3-deck2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boxed-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ori/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "naughty-narrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1095951, 4]), torch.Size([1095951, 4]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distdeck[\"dist\"].shape, distdeck[\"level\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rolled-theorem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['dist', 'level']), dict_keys(['feat', 'otext', 'fclevel', 'ids']))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distdeck.keys(), deck2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handled-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 1418916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "worse-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(dists, tlevels, fclevel):\n",
    "    if fclevel in [6, 4, 2]:\n",
    "        return fclevel.item()\n",
    "    if (tlevels == 5).all():\n",
    "        return 5 if dists[0] < 1.5 else 7\n",
    "    if (tlevels == 3).all():\n",
    "        return 3 if dists[0] < 1.5 else 7\n",
    "    if dists[0] < 0.7:\n",
    "        return fclevel.item()\n",
    "    return 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "immediate-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1095951/1095951 [01:20<00:00, 13610.90it/s]\n"
     ]
    }
   ],
   "source": [
    "out_dists = [None for _ in range(total_len)]\n",
    "out_levels = [None for _ in range(total_len)]\n",
    "out_fclevels = [None for _ in range(total_len)]\n",
    "N = distdeck[\"dist\"].size(0)\n",
    "with tqdm(total=N, ncols=100, file=sys.stdout) as t:\n",
    "    for i in range(N):\n",
    "        dists = distdeck[\"dist\"][i]\n",
    "        levels = distdeck[\"level\"][i]\n",
    "        fclevel = deck2[\"fclevel\"][i]\n",
    "        out_level = policy(dists, levels, fclevel)\n",
    "        ids = deck2[\"ids\"][i]\n",
    "        for j in ids:\n",
    "            out_levels[j - 1000000] = out_level\n",
    "            out_dists[j - 1000000] = dists\n",
    "            out_fclevels[j - 1000000] = fclevel\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inner-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_levels = np.array(out_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "architectural-cricket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 1003955 70.76%\n",
      "1 : 395007 27.84%\n",
      "2 : 42 0.00%\n",
      "3 : 12950 0.91%\n",
      "4 : 34 0.00%\n",
      "5 : 6334 0.45%\n",
      "6 : 31 0.00%\n",
      "7 : 563 0.04%\n"
     ]
    }
   ],
   "source": [
    "# 각 레벨별 개수\n",
    "for i in range(8):\n",
    "    cnt = (out_levels == i).sum()\n",
    "    print(i, \":\", cnt, f\"{cnt / len(out_levels)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "widespread-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ids = list(range(1000000, 1000000 + len(out_levels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "developmental-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = {\"id\": out_ids, \"level\": out_levels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "binary-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "expired-conservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418911</th>\n",
       "      <td>2418911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418912</th>\n",
       "      <td>2418912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418913</th>\n",
       "      <td>2418913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418914</th>\n",
       "      <td>2418914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418915</th>\n",
       "      <td>2418915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1418916 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  level\n",
       "0        1000000      0\n",
       "1        1000001      0\n",
       "2        1000002      1\n",
       "3        1000003      0\n",
       "4        1000004      1\n",
       "...          ...    ...\n",
       "1418911  2418911      0\n",
       "1418912  2418912      0\n",
       "1418913  2418913      1\n",
       "1418914  2418914      0\n",
       "1418915  2418915      0\n",
       "\n",
       "[1418916 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deadly-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 출력\n",
    "out_df.to_csv(\n",
    "    \"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3-out_ver2.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75706c-0974-4c89-906b-88b2ed4e75a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e69683-a795-42c3-8d1d-daa5e46abf7b",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996a5358-ce26-4c44-a717-7d0ec95d3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs = [\n",
    "    pd.read_csv(\"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_3-out_ver2.csv\"),\n",
    "    \n",
    "    pd.read_csv(\"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_1-out_ver2.csv\"),\n",
    "    pd.read_csv(\"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_2-out_ver2.csv\"),\n",
    "    pd.read_csv(\"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_4-out_ver2.csv\"),\n",
    "    pd.read_csv(\"results/distilbert-base-uncased-ver7/distilbert-base-uncased-focal-AdamW-lr1e-05-ver7-os10_5-out_ver2.csv\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a215da-1344-45d7-bd5e-fd2a436649c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = np.stack([csv.level.to_numpy() for csv in csvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3f8802-51d9-4818-b04b-fced0b279eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(csvs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f2f6a5-a419-4b08-a69a-359f589c7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_levels = []\n",
    "for i in range(N):\n",
    "    kt = False\n",
    "    ll = [0 for _ in range(8)]\n",
    "    for j in range(len(csvs)):\n",
    "        if levels[j, i] in [2, 4, 6]:\n",
    "            out_levels.append(levels[j, i])\n",
    "            kt = True\n",
    "            break\n",
    "        else:\n",
    "            ll[levels[j, i]] += 1\n",
    "\n",
    "    if not kt:\n",
    "        if max(ll) == 1:\n",
    "            out_levels.append(levels[0, i])\n",
    "        elif ll[7] >= 2:\n",
    "            out_levels.append(7)\n",
    "        else:\n",
    "            out_levels.append(max(range(8), key=lambda i: ll[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904acc4e-d1a9-4fa2-9506-815260652993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": csvs[0].id.to_list(), \"level\": out_levels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6550bbd9-ca13-464d-948e-eb008df2f2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418911</th>\n",
       "      <td>2418911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418912</th>\n",
       "      <td>2418912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418913</th>\n",
       "      <td>2418913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418914</th>\n",
       "      <td>2418914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418915</th>\n",
       "      <td>2418915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1418916 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  level\n",
       "0        1000000      0\n",
       "1        1000001      0\n",
       "2        1000002      1\n",
       "3        1000003      0\n",
       "4        1000004      1\n",
       "...          ...    ...\n",
       "1418911  2418911      0\n",
       "1418912  2418912      0\n",
       "1418913  2418913      1\n",
       "1418914  2418914      0\n",
       "1418915  2418915      0\n",
       "\n",
       "[1418916 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caad218e-8f74-44e5-a99d-ce69abd47288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results/ens-ver2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-trust",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
